#!/usr/bin/env python3
"""
Polymarket æ•°æ®åˆ†æå™¨
åˆ†æåŒæ­¥çš„å¸‚åœºæ•°æ®ï¼Œç”Ÿæˆæ´å¯ŸæŠ¥å‘Š
"""

import os
import json
import glob
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging
from collections import defaultdict, Counter

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataAnalyzer:
    """æ•°æ®åˆ†æå™¨"""
    
    def __init__(self, data_dir: str = "./data"):
        self.data_dir = data_dir
        self.tag_dir = os.path.join(data_dir, "tag")
        self.reports_dir = os.path.join(data_dir, "reports")
        self.analysis_dir = os.path.join(data_dir, "analysis")
        
        # ç¡®ä¿åˆ†æç›®å½•å­˜åœ¨
        os.makedirs(self.analysis_dir, exist_ok=True)
    
    def load_all_events_data(self) -> pd.DataFrame:
        """åŠ è½½æ‰€æœ‰äº‹ä»¶æ•°æ®"""
        all_events = []
        
        if not os.path.exists(self.tag_dir):
            logger.warning("æ ‡ç­¾ç›®å½•ä¸å­˜åœ¨")
            return pd.DataFrame()
        
        # éå†æ‰€æœ‰æ ‡ç­¾ç›®å½•
        for tag_name in os.listdir(self.tag_dir):
            tag_path = os.path.join(self.tag_dir, tag_name)
            if not os.path.isdir(tag_path):
                continue
            
            # æŸ¥æ‰¾æœ€æ–°çš„äº‹ä»¶æ–‡ä»¶
            events_files = glob.glob(os.path.join(tag_path, "events_*.csv"))
            if not events_files:
                continue
            
            latest_file = max(events_files, key=os.path.getmtime)
            
            try:
                df = pd.read_csv(latest_file)
                df['tag'] = tag_name  # æ·»åŠ æ ‡ç­¾ä¿¡æ¯
                all_events.append(df)
            except Exception as e:
                logger.error(f"è¯»å–æ–‡ä»¶ {latest_file} å¤±è´¥: {e}")
        
        if all_events:
            combined_df = pd.concat(all_events, ignore_index=True)
            logger.info(f"åŠ è½½äº† {len(combined_df)} ä¸ªäº‹ä»¶æ•°æ®")
            return combined_df
        else:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°äº‹ä»¶æ•°æ®")
            return pd.DataFrame()
    
    def load_all_markets_data(self) -> pd.DataFrame:
        """åŠ è½½æ‰€æœ‰å¸‚åœºæ•°æ®"""
        all_markets = []
        
        if not os.path.exists(self.tag_dir):
            logger.warning("æ ‡ç­¾ç›®å½•ä¸å­˜åœ¨")
            return pd.DataFrame()
        
        # éå†æ‰€æœ‰æ ‡ç­¾ç›®å½•
        for tag_name in os.listdir(self.tag_dir):
            tag_path = os.path.join(self.tag_dir, tag_name)
            if not os.path.isdir(tag_path):
                continue
            
            # æŸ¥æ‰¾æœ€æ–°çš„å¸‚åœºæ–‡ä»¶
            markets_files = glob.glob(os.path.join(tag_path, "markets_*.csv"))
            if not markets_files:
                continue
            
            latest_file = max(markets_files, key=os.path.getmtime)
            
            try:
                df = pd.read_csv(latest_file)
                df['tag'] = tag_name  # æ·»åŠ æ ‡ç­¾ä¿¡æ¯
                all_markets.append(df)
            except Exception as e:
                logger.error(f"è¯»å–æ–‡ä»¶ {latest_file} å¤±è´¥: {e}")
        
        if all_markets:
            combined_df = pd.concat(all_markets, ignore_index=True)
            logger.info(f"åŠ è½½äº† {len(combined_df)} ä¸ªå¸‚åœºæ•°æ®")
            return combined_df
        else:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°å¸‚åœºæ•°æ®")
            return pd.DataFrame()
    
    def analyze_market_distribution(self, events_df: pd.DataFrame) -> Dict:
        """åˆ†æå¸‚åœºåˆ†å¸ƒ"""
        if events_df.empty:
            return {}
        
        analysis = {
            'total_events': int(len(events_df)),
            'total_volume': float(events_df['volume'].sum()),
            'total_liquidity': float(events_df['liquidity'].sum()),
            'average_volume': float(events_df['volume'].mean()),
            'median_volume': float(events_df['volume'].median()),
            'volume_std': float(events_df['volume'].std()),
            
            # æŒ‰æ ‡ç­¾åˆ†å¸ƒ
            'distribution_by_tag': {},
            'top_tags_by_volume': [],
            'top_tags_by_count': [],
            
            # æŒ‰ç±»åˆ«åˆ†å¸ƒ
            'distribution_by_category': {},
            
            # æ—¶é—´åˆ†æ
            'time_analysis': {}
        }
        
        # æŒ‰æ ‡ç­¾åˆ†æ
        tag_stats = events_df.groupby('tag').agg({
            'volume': ['count', 'sum', 'mean'],
            'liquidity': 'sum'
        }).round(2)
        
        for tag in tag_stats.index:
            analysis['distribution_by_tag'][tag] = {
                'count': int(tag_stats.loc[tag, ('volume', 'count')]),
                'total_volume': float(tag_stats.loc[tag, ('volume', 'sum')]),
                'avg_volume': float(tag_stats.loc[tag, ('volume', 'mean')]),
                'total_liquidity': float(tag_stats.loc[tag, ('liquidity', 'sum')])
            }
        
        # çƒ­é—¨æ ‡ç­¾
        analysis['top_tags_by_volume'] = sorted(
            analysis['distribution_by_tag'].items(),
            key=lambda x: x[1]['total_volume'],
            reverse=True
        )[:10]
        
        analysis['top_tags_by_count'] = sorted(
            analysis['distribution_by_tag'].items(),
            key=lambda x: x[1]['count'],
            reverse=True
        )[:10]
        
        # æŒ‰ç±»åˆ«åˆ†æ
        if 'category' in events_df.columns:
            category_stats = events_df.groupby('category').agg({
                'volume': ['count', 'sum', 'mean'],
                'liquidity': 'sum'
            }).round(2)
            
            for category in category_stats.index:
                analysis['distribution_by_category'][category] = {
                    'count': int(category_stats.loc[category, ('volume', 'count')]),
                    'total_volume': float(category_stats.loc[category, ('volume', 'sum')]),
                    'avg_volume': float(category_stats.loc[category, ('volume', 'mean')]),
                    'total_liquidity': float(category_stats.loc[category, ('liquidity', 'sum')])
                }
        
        # æ—¶é—´åˆ†æ
        if 'end_date' in events_df.columns:
            try:
                events_df['end_date_parsed'] = pd.to_datetime(events_df['end_date'], utc=True)
                now = pd.Timestamp.now(tz='UTC')
                
                # æŒ‰åˆ°æœŸæ—¶é—´åˆ†ç»„
                events_df['days_to_expiry'] = (events_df['end_date_parsed'] - now).dt.days
                
                analysis['time_analysis'] = {
                    'expiring_soon': int(len(events_df[events_df['days_to_expiry'] <= 7])),
                    'expiring_this_month': int(len(events_df[events_df['days_to_expiry'] <= 30])),
                    'expiring_this_year': int(len(events_df[events_df['days_to_expiry'] <= 365])),
                    'average_days_to_expiry': float(events_df['days_to_expiry'].mean()),
                    'median_days_to_expiry': float(events_df['days_to_expiry'].median())
                }
            except Exception as e:
                logger.error(f"æ—¶é—´åˆ†æå¤±è´¥: {e}")
                analysis['time_analysis'] = {'error': str(e)}
        
        return analysis
    
    def analyze_market_trends(self, events_df: pd.DataFrame) -> Dict:
        """åˆ†æå¸‚åœºè¶‹åŠ¿"""
        if events_df.empty:
            return {}
        
        trends = {
            'volume_trends': {},
            'liquidity_trends': {},
            'growth_analysis': {},
            'correlation_analysis': {}
        }
        
        # äº¤æ˜“é‡è¶‹åŠ¿
        volume_percentiles = np.percentile(events_df['volume'], [25, 50, 75, 90, 95])
        trends['volume_trends'] = {
            'q25': float(volume_percentiles[0]),
            'median': float(volume_percentiles[1]),
            'q75': float(volume_percentiles[2]),
            'p90': float(volume_percentiles[3]),
            'p95': float(volume_percentiles[4]),
            'high_volume_events': len(events_df[events_df['volume'] > volume_percentiles[3]]),
            'low_volume_events': len(events_df[events_df['volume'] < volume_percentiles[0]])
        }
        
        # æµåŠ¨æ€§è¶‹åŠ¿
        liquidity_percentiles = np.percentile(events_df['liquidity'], [25, 50, 75, 90, 95])
        trends['liquidity_trends'] = {
            'q25': float(liquidity_percentiles[0]),
            'median': float(liquidity_percentiles[1]),
            'q75': float(liquidity_percentiles[2]),
            'p90': float(liquidity_percentiles[3]),
            'p95': float(liquidity_percentiles[4]),
            'high_liquidity_events': len(events_df[events_df['liquidity'] > liquidity_percentiles[3]]),
            'low_liquidity_events': len(events_df[events_df['liquidity'] < liquidity_percentiles[0]])
        }
        
        # å¢é•¿åˆ†æ
        if len(events_df) > 1:
            volume_growth = events_df['volume'].pct_change().dropna()
            trends['growth_analysis'] = {
                'volume_volatility': float(volume_growth.std()),
                'positive_growth_events': len(volume_growth[volume_growth > 0]),
                'negative_growth_events': len(volume_growth[volume_growth < 0])
            }
        
        # ç›¸å…³æ€§åˆ†æ
        numeric_columns = ['volume', 'liquidity']
        if all(col in events_df.columns for col in numeric_columns):
            correlation_matrix = events_df[numeric_columns].corr()
            trends['correlation_analysis'] = {
                'volume_liquidity_correlation': float(correlation_matrix.loc['volume', 'liquidity'])
            }
        
        return trends
    
    def identify_opportunities(self, events_df: pd.DataFrame, markets_df: pd.DataFrame) -> Dict:
        """è¯†åˆ«äº¤æ˜“æœºä¼š"""
        opportunities = {
            'high_volume_low_liquidity': [],
            'emerging_trends': [],
            'undervalued_markets': [],
            'arbitrage_opportunities': [],
            'risk_warnings': []
        }
        
        if events_df.empty:
            return opportunities
        
        # é«˜äº¤æ˜“é‡ä½æµåŠ¨æ€§æœºä¼š
        volume_threshold = events_df['volume'].quantile(0.8)
        liquidity_threshold = events_df['liquidity'].quantile(0.3)
        
        high_vol_low_liq = events_df[
            (events_df['volume'] > volume_threshold) & 
            (events_df['liquidity'] < liquidity_threshold)
        ]
        
        for _, event in high_vol_low_liq.iterrows():
            opportunities['high_volume_low_liquidity'].append({
                'title': event.get('title', 'N/A'),
                'tag': event.get('tag', 'N/A'),
                'volume': float(event['volume']),
                'liquidity': float(event['liquidity']),
                'volume_to_liquidity_ratio': float(event['volume'] / max(event['liquidity'], 1))
            })
        
        # æ–°å…´è¶‹åŠ¿è¯†åˆ«
        tag_volumes = events_df.groupby('tag')['volume'].sum().sort_values(ascending=False)
        emerging_tags = tag_volumes.head(5)
        
        for tag, volume in emerging_tags.items():
            tag_events = events_df[events_df['tag'] == tag]
            opportunities['emerging_trends'].append({
                'tag': tag,
                'total_volume': float(volume),
                'event_count': len(tag_events),
                'avg_volume': float(tag_events['volume'].mean()),
                'growth_potential': 'high' if volume > events_df['volume'].mean() * 2 else 'medium'
            })
        
        # é£é™©è­¦å‘Š
        # ä½æµåŠ¨æ€§è­¦å‘Š
        very_low_liquidity = events_df[events_df['liquidity'] < events_df['liquidity'].quantile(0.1)]
        for _, event in very_low_liquidity.head(5).iterrows():
            opportunities['risk_warnings'].append({
                'type': 'low_liquidity',
                'title': event.get('title', 'N/A'),
                'tag': event.get('tag', 'N/A'),
                'liquidity': float(event['liquidity']),
                'warning': 'æµåŠ¨æ€§æä½ï¼Œå¯èƒ½éš¾ä»¥é€€å‡º'
            })
        
        # å¼‚å¸¸é«˜äº¤æ˜“é‡è­¦å‘Š
        very_high_volume = events_df[events_df['volume'] > events_df['volume'].quantile(0.95)]
        for _, event in very_high_volume.head(3).iterrows():
            opportunities['risk_warnings'].append({
                'type': 'high_volume',
                'title': event.get('title', 'N/A'),
                'tag': event.get('tag', 'N/A'),
                'volume': float(event['volume']),
                'warning': 'äº¤æ˜“é‡å¼‚å¸¸é«˜ï¼Œå¯èƒ½å­˜åœ¨å¸‚åœºæ“çºµ'
            })
        
        return opportunities
    
    def generate_insights_report(self, events_df: pd.DataFrame, markets_df: pd.DataFrame) -> Dict:
        """ç”Ÿæˆæ´å¯ŸæŠ¥å‘Š"""
        logger.info("ç”Ÿæˆæ•°æ®æ´å¯ŸæŠ¥å‘Š...")
        
        report = {
            'report_timestamp': datetime.now().isoformat(),
            'data_summary': {
                'events_count': len(events_df),
                'markets_count': len(markets_df),
                'unique_tags': events_df['tag'].nunique() if not events_df.empty else 0,
                'data_freshness': self.calculate_data_freshness(events_df)
            },
            'market_distribution': self.analyze_market_distribution(events_df),
            'market_trends': self.analyze_market_trends(events_df),
            'opportunities': self.identify_opportunities(events_df, markets_df),
            'recommendations': self.generate_recommendations(events_df, markets_df)
        }
        
        return report
    
    def calculate_data_freshness(self, events_df: pd.DataFrame) -> Dict:
        """è®¡ç®—æ•°æ®æ–°é²œåº¦"""
        if events_df.empty or 'sync_timestamp' not in events_df.columns:
            return {'status': 'unknown', 'message': 'æ— æ³•ç¡®å®šæ•°æ®æ–°é²œåº¦'}
        
        try:
            latest_sync = pd.to_datetime(events_df['sync_timestamp']).max()
            age_hours = (pd.Timestamp.now() - latest_sync).total_seconds() / 3600
            
            if age_hours < 1:
                status = 'very_fresh'
                message = f'{age_hours:.1f} å°æ—¶å‰æ›´æ–°'
            elif age_hours < 6:
                status = 'fresh'
                message = f'{age_hours:.1f} å°æ—¶å‰æ›´æ–°'
            elif age_hours < 24:
                status = 'stale'
                message = f'{age_hours:.1f} å°æ—¶å‰æ›´æ–°'
            else:
                status = 'old'
                message = f'{age_hours/24:.1f} å¤©å‰æ›´æ–°'
            
            return {
                'status': status,
                'age_hours': age_hours,
                'message': message,
                'last_sync': latest_sync.isoformat()
            }
        except Exception as e:
            return {'status': 'error', 'message': f'è®¡ç®—å¤±è´¥: {e}'}
    
    def generate_recommendations(self, events_df: pd.DataFrame, markets_df: pd.DataFrame) -> List[Dict]:
        """ç”Ÿæˆæ¨èå»ºè®®"""
        recommendations = []
        
        if events_df.empty:
            return [{'type': 'warning', 'message': 'æ²¡æœ‰æ•°æ®å¯ä¾›åˆ†æ'}]
        
        # åŸºäºäº¤æ˜“é‡çš„å»ºè®®
        high_volume_tags = events_df.groupby('tag')['volume'].sum().nlargest(3)
        for tag, volume in high_volume_tags.items():
            recommendations.append({
                'type': 'opportunity',
                'category': 'high_volume',
                'message': f'æ ‡ç­¾ "{tag}" äº¤æ˜“é‡è¾ƒé«˜ (${volume:,.0f})ï¼Œå€¼å¾—å…³æ³¨',
                'priority': 'high',
                'action': f'æ·±å…¥åˆ†æ {tag} ç›¸å…³å¸‚åœº'
            })
        
        # åŸºäºæµåŠ¨æ€§çš„å»ºè®®
        low_liquidity_count = len(events_df[events_df['liquidity'] < events_df['liquidity'].quantile(0.2)])
        if low_liquidity_count > len(events_df) * 0.3:
            recommendations.append({
                'type': 'warning',
                'category': 'liquidity',
                'message': f'{low_liquidity_count} ä¸ªäº‹ä»¶æµåŠ¨æ€§è¾ƒä½ï¼Œäº¤æ˜“æ—¶éœ€è°¨æ…',
                'priority': 'medium',
                'action': 'é¿å…å¤§é¢äº¤æ˜“ï¼Œæˆ–ç­‰å¾…æµåŠ¨æ€§æ”¹å–„'
            })
        
        # åŸºäºå¤šæ ·æ€§çš„å»ºè®®
        tag_diversity = events_df['tag'].nunique()
        if tag_diversity < 5:
            recommendations.append({
                'type': 'suggestion',
                'category': 'diversification',
                'message': f'å½“å‰åªæœ‰ {tag_diversity} ä¸ªæ ‡ç­¾ï¼Œå»ºè®®æ‰©å¤§å…³æ³¨èŒƒå›´',
                'priority': 'low',
                'action': 'å…³æ³¨æ›´å¤šç±»åˆ«çš„å¸‚åœºä»¥åˆ†æ•£é£é™©'
            })
        
        # åŸºäºæ—¶é—´çš„å»ºè®®
        if 'end_date' in events_df.columns:
            try:
                events_df['end_date_parsed'] = pd.to_datetime(events_df['end_date'], utc=True)
                expiring_soon = len(events_df[
                    (events_df['end_date_parsed'] - pd.Timestamp.now(tz='UTC')).dt.days <= 7
                ])
                
                if expiring_soon > 0:
                    recommendations.append({
                        'type': 'urgent',
                        'category': 'timing',
                        'message': f'{expiring_soon} ä¸ªäº‹ä»¶å°†åœ¨ä¸€å‘¨å†…åˆ°æœŸ',
                        'priority': 'high',
                        'action': 'å°½å¿«å†³å®šæ˜¯å¦å‚ä¸å³å°†åˆ°æœŸçš„å¸‚åœº'
                    })
            except Exception:
                pass
        
        return recommendations
    
    def save_analysis_report(self, report: Dict, filename: str = None) -> str:
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"analysis_report_{timestamp}.json"
        
        filepath = os.path.join(self.analysis_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath}")
        return filepath
    
    def generate_text_report(self, report: Dict) -> str:
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
        summary = report['data_summary']
        distribution = report['market_distribution']
        trends = report['market_trends']
        opportunities = report['opportunities']
        recommendations = report['recommendations']
        
        text_report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    Polymarket æ•°æ®åˆ†ææŠ¥å‘Š                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ“Š æ•°æ®æ¦‚è§ˆ                                                  â•‘
â•‘   äº‹ä»¶æ•°é‡: {summary['events_count']:,}                     â•‘
â•‘   å¸‚åœºæ•°é‡: {summary['markets_count']:,}                    â•‘
â•‘   æ ‡ç­¾æ•°é‡: {summary['unique_tags']}                        â•‘
â•‘   æ•°æ®æ–°é²œåº¦: {summary['data_freshness']['message']}        â•‘
â•‘                                                              â•‘
â•‘ ğŸ’° å¸‚åœºåˆ†å¸ƒ                                                  â•‘
â•‘   æ€»äº¤æ˜“é‡: ${distribution.get('total_volume', 0):,.0f}     â•‘
â•‘   æ€»æµåŠ¨æ€§: ${distribution.get('total_liquidity', 0):,.0f}  â•‘
â•‘   å¹³å‡äº¤æ˜“é‡: ${distribution.get('average_volume', 0):,.0f} â•‘
â•‘                                                              â•‘
â•‘ ğŸ·ï¸  çƒ­é—¨æ ‡ç­¾ (æŒ‰äº¤æ˜“é‡)                                      â•‘"""
        
        for i, (tag, stats) in enumerate(distribution.get('top_tags_by_volume', [])[:5], 1):
            text_report += f"""
â•‘   {i}. {tag}: ${stats['total_volume']:,.0f}                 â•‘"""
        
        text_report += f"""
â•‘                                                              â•‘
â•‘ ğŸ“ˆ å¸‚åœºè¶‹åŠ¿                                                  â•‘
â•‘   äº¤æ˜“é‡ä¸­ä½æ•°: ${trends.get('volume_trends', {}).get('median', 0):,.0f} â•‘
â•‘   é«˜äº¤æ˜“é‡äº‹ä»¶: {trends.get('volume_trends', {}).get('high_volume_events', 0)} ä¸ª â•‘
â•‘   æµåŠ¨æ€§ä¸­ä½æ•°: ${trends.get('liquidity_trends', {}).get('median', 0):,.0f} â•‘
â•‘                                                              â•‘
â•‘ ğŸ¯ äº¤æ˜“æœºä¼š                                                  â•‘
â•‘   é«˜é‡ä½æµåŠ¨æ€§: {len(opportunities.get('high_volume_low_liquidity', []))} ä¸ª â•‘
â•‘   æ–°å…´è¶‹åŠ¿: {len(opportunities.get('emerging_trends', []))} ä¸ª     â•‘
â•‘   é£é™©è­¦å‘Š: {len(opportunities.get('risk_warnings', []))} ä¸ª       â•‘
â•‘                                                              â•‘
â•‘ ğŸ’¡ æ¨èå»ºè®®                                                  â•‘"""
        
        for i, rec in enumerate(recommendations[:3], 1):
            priority_icon = "ğŸ”´" if rec['priority'] == 'high' else "ğŸŸ¡" if rec['priority'] == 'medium' else "ğŸŸ¢"
            text_report += f"""
â•‘   {i}. {priority_icon} {rec['message'][:40]}...              â•‘"""
        
        text_report += f"""
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        
        return text_report.strip()
    
    def run_full_analysis(self) -> Tuple[Dict, str]:
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        logger.info("å¼€å§‹å®Œæ•´æ•°æ®åˆ†æ...")
        
        # åŠ è½½æ•°æ®
        events_df = self.load_all_events_data()
        markets_df = self.load_all_markets_data()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_insights_report(events_df, markets_df)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.save_analysis_report(report)
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        text_report = self.generate_text_report(report)
        
        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        text_file = os.path.join(self.analysis_dir, f"analysis_report_{timestamp}.txt")
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(text_report)
        
        logger.info("æ•°æ®åˆ†æå®Œæˆ")
        return report, text_report

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Polymarketæ•°æ®åˆ†æå™¨')
    parser.add_argument('--data-dir', default='./data', help='æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--output', choices=['json', 'text', 'both'], default='both', help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--save', action='store_true', help='ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶')
    
    args = parser.parse_args()
    
    analyzer = DataAnalyzer(data_dir=args.data_dir)
    
    # è¿è¡Œåˆ†æ
    report, text_report = analyzer.run_full_analysis()
    
    # è¾“å‡ºç»“æœ
    if args.output in ['text', 'both']:
        print(text_report)
    
    if args.output in ['json', 'both']:
        print(f"\nè¯¦ç»†JSONæŠ¥å‘Šå·²ä¿å­˜åˆ°åˆ†æç›®å½•")
    
    if not args.save:
        print(f"\nğŸ“ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {analyzer.analysis_dir}")

if __name__ == "__main__":
    main()