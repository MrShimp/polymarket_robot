#!/usr/bin/env python3
"""
åŒæ­¥æ€§èƒ½æµ‹è¯•å·¥å…· - æµ‹è¯•ä¸åŒæ¨¡å¼ä¸‹çš„åŒæ­¥æ—¶é—´å’Œæ€§èƒ½
"""

import time
import json
import os
from datetime import datetime
from typing import Dict, Any, List
from sync.enhanced_sync import EnhancedPolymarketSync
from sync.offline_data_generator import OfflineDataGenerator

class SyncPerformanceTest:
    def __init__(self, data_dir: str = "./data"):
        self.data_dir = data_dir
        self.results = []
    
    def test_offline_sync(self, iterations: int = 3) -> Dict[str, Any]:
        """æµ‹è¯•ç¦»çº¿æ¨¡å¼åŒæ­¥æ€§èƒ½"""
        print("ğŸ”„ æµ‹è¯•ç¦»çº¿æ¨¡å¼åŒæ­¥æ€§èƒ½...")
        
        times = []
        data_stats = []
        
        for i in range(iterations):
            print(f"   ç¬¬ {i+1}/{iterations} æ¬¡æµ‹è¯•...")
            
            syncer = EnhancedPolymarketSync(self.data_dir, offline_mode=True)
            
            start_time = time.time()
            report = syncer.sync_all_data()
            end_time = time.time()
            
            duration = end_time - start_time
            times.append(duration)
            
            data_stats.append({
                "events": report.get("events_count", 0),
                "markets": report.get("markets_count", 0),
                "tags": report.get("tags_count", 0)
            })
        
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        
        # è·å–æ•°æ®ç»Ÿè®¡
        last_stats = data_stats[-1] if data_stats else {}
        
        result = {
            "mode": "offline",
            "iterations": iterations,
            "avg_time": avg_time,
            "min_time": min_time,
            "max_time": max_time,
            "times": times,
            "events_count": last_stats.get("events", 0),
            "markets_count": last_stats.get("markets", 0),
            "tags_count": last_stats.get("tags", 0),
            "throughput": {
                "events_per_second": last_stats.get("events", 0) / avg_time,
                "markets_per_second": last_stats.get("markets", 0) / avg_time
            }
        }
        
        self.results.append(result)
        return result
    
    def test_api_sync(self, timeout: int = 60) -> Dict[str, Any]:
        """æµ‹è¯•APIæ¨¡å¼åŒæ­¥æ€§èƒ½"""
        print("ğŸ”„ æµ‹è¯•APIæ¨¡å¼åŒæ­¥æ€§èƒ½...")
        
        syncer = EnhancedPolymarketSync(self.data_dir, offline_mode=False)
        
        try:
            start_time = time.time()
            report = syncer.sync_all_data()
            end_time = time.time()
            
            duration = end_time - start_time
            
            result = {
                "mode": "api",
                "success": True,
                "duration": duration,
                "events_count": report.get("events_count", 0),
                "markets_count": report.get("markets_count", 0),
                "tags_count": report.get("tags_count", 0),
                "throughput": {
                    "events_per_second": report.get("events_count", 0) / duration if duration > 0 else 0,
                    "markets_per_second": report.get("markets_count", 0) / duration if duration > 0 else 0
                }
            }
            
        except Exception as e:
            result = {
                "mode": "api",
                "success": False,
                "error": str(e),
                "duration": 0
            }
        
        self.results.append(result)
        return result
    
    def test_data_generation(self) -> Dict[str, Any]:
        """æµ‹è¯•ç¦»çº¿æ•°æ®ç”Ÿæˆæ€§èƒ½"""
        print("ğŸ”„ æµ‹è¯•ç¦»çº¿æ•°æ®ç”Ÿæˆæ€§èƒ½...")
        
        generator = OfflineDataGenerator()
        
        start_time = time.time()
        tags, events, markets = generator.save_offline_data(f"{self.data_dir}/offline")
        end_time = time.time()
        
        duration = end_time - start_time
        
        result = {
            "mode": "data_generation",
            "duration": duration,
            "tags_generated": len(tags),
            "events_generated": len(events),
            "markets_generated": len(markets),
            "throughput": {
                "events_per_second": len(events) / duration,
                "markets_per_second": len(markets) / duration
            }
        }
        
        self.results.append(result)
        return result
    
    def benchmark_file_operations(self) -> Dict[str, Any]:
        """æµ‹è¯•æ–‡ä»¶æ“ä½œæ€§èƒ½"""
        print("ğŸ”„ æµ‹è¯•æ–‡ä»¶æ“ä½œæ€§èƒ½...")
        
        # æµ‹è¯•æ•°æ®è¯»å–é€Ÿåº¦
        tag_dir = os.path.join(self.data_dir, "tag")
        if not os.path.exists(tag_dir):
            return {"mode": "file_ops", "error": "No data to test"}
        
        start_time = time.time()
        
        file_count = 0
        total_size = 0
        
        for tag_name in os.listdir(tag_dir):
            tag_path = os.path.join(tag_dir, tag_name)
            if os.path.isdir(tag_path):
                for file_name in os.listdir(tag_path):
                    file_path = os.path.join(tag_path, file_name)
                    if os.path.isfile(file_path):
                        file_count += 1
                        total_size += os.path.getsize(file_path)
        
        end_time = time.time()
        duration = end_time - start_time
        
        result = {
            "mode": "file_operations",
            "duration": duration,
            "files_scanned": file_count,
            "total_size_mb": total_size / (1024 * 1024),
            "throughput": {
                "files_per_second": file_count / duration if duration > 0 else 0,
                "mb_per_second": (total_size / (1024 * 1024)) / duration if duration > 0 else 0
            }
        }
        
        self.results.append(result)
        return result
    
    def run_full_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹å®Œæ•´æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        print("=" * 60)
        
        benchmark_start = time.time()
        
        # 1. æµ‹è¯•æ•°æ®ç”Ÿæˆ
        gen_result = self.test_data_generation()
        
        # 2. æµ‹è¯•ç¦»çº¿åŒæ­¥ (å¤šæ¬¡)
        offline_result = self.test_offline_sync(iterations=5)
        
        # 3. æµ‹è¯•APIåŒæ­¥ (å¦‚æœå¯ç”¨)
        api_result = self.test_api_sync()
        
        # 4. æµ‹è¯•æ–‡ä»¶æ“ä½œ
        file_result = self.benchmark_file_operations()
        
        benchmark_end = time.time()
        total_benchmark_time = benchmark_end - benchmark_start
        
        # æ±‡æ€»ç»“æœ
        summary = {
            "benchmark_time": total_benchmark_time,
            "timestamp": datetime.now().isoformat(),
            "results": {
                "data_generation": gen_result,
                "offline_sync": offline_result,
                "api_sync": api_result,
                "file_operations": file_result
            }
        }
        
        return summary
    
    def print_results(self, summary: Dict[str, Any]):
        """æ‰“å°æ€§èƒ½æµ‹è¯•ç»“æœ"""
        print("\n" + "="*80)
        print("ğŸ“Š Polymarket åŒæ­¥æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        print(f"ğŸ• æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â±ï¸  æ€»æµ‹è¯•è€—æ—¶: {summary['benchmark_time']:.2f} ç§’")
        print()
        
        results = summary["results"]
        
        # æ•°æ®ç”Ÿæˆæ€§èƒ½
        if "data_generation" in results:
            gen = results["data_generation"]
            print("ğŸ”§ ç¦»çº¿æ•°æ®ç”Ÿæˆæ€§èƒ½:")
            print(f"   è€—æ—¶: {gen['duration']:.3f} ç§’")
            print(f"   ç”Ÿæˆ: {gen['tags_generated']} æ ‡ç­¾, {gen['events_generated']} äº‹ä»¶, {gen['markets_generated']} å¸‚åœº")
            print(f"   é€Ÿåº¦: {gen['throughput']['events_per_second']:.1f} äº‹ä»¶/ç§’, {gen['throughput']['markets_per_second']:.1f} å¸‚åœº/ç§’")
            print()
        
        # ç¦»çº¿åŒæ­¥æ€§èƒ½
        if "offline_sync" in results:
            offline = results["offline_sync"]
            print("ğŸ”„ ç¦»çº¿æ¨¡å¼åŒæ­¥æ€§èƒ½:")
            print(f"   æµ‹è¯•æ¬¡æ•°: {offline['iterations']} æ¬¡")
            print(f"   å¹³å‡è€—æ—¶: {offline['avg_time']:.3f} ç§’")
            print(f"   æœ€å¿«: {offline['min_time']:.3f} ç§’, æœ€æ…¢: {offline['max_time']:.3f} ç§’")
            print(f"   æ•°æ®é‡: {offline['events_count']} äº‹ä»¶, {offline['markets_count']} å¸‚åœº, {offline['tags_count']} æ ‡ç­¾")
            print(f"   ååé‡: {offline['throughput']['events_per_second']:.1f} äº‹ä»¶/ç§’, {offline['throughput']['markets_per_second']:.1f} å¸‚åœº/ç§’")
            print()
        
        # APIåŒæ­¥æ€§èƒ½
        if "api_sync" in results:
            api = results["api_sync"]
            print("ğŸŒ APIæ¨¡å¼åŒæ­¥æ€§èƒ½:")
            if api.get("success"):
                print(f"   è€—æ—¶: {api['duration']:.3f} ç§’")
                print(f"   æ•°æ®é‡: {api['events_count']} äº‹ä»¶, {api['markets_count']} å¸‚åœº, {api['tags_count']} æ ‡ç­¾")
                print(f"   ååé‡: {api['throughput']['events_per_second']:.1f} äº‹ä»¶/ç§’, {api['throughput']['markets_per_second']:.1f} å¸‚åœº/ç§’")
            else:
                print(f"   âŒ åŒæ­¥å¤±è´¥: {api.get('error', 'æœªçŸ¥é”™è¯¯')}")
            print()
        
        # æ–‡ä»¶æ“ä½œæ€§èƒ½
        if "file_operations" in results:
            file_ops = results["file_operations"]
            if "error" not in file_ops:
                print("ğŸ“ æ–‡ä»¶æ“ä½œæ€§èƒ½:")
                print(f"   æ‰«æè€—æ—¶: {file_ops['duration']:.3f} ç§’")
                print(f"   æ–‡ä»¶æ•°é‡: {file_ops['files_scanned']} ä¸ª")
                print(f"   æ€»å¤§å°: {file_ops['total_size_mb']:.2f} MB")
                print(f"   é€Ÿåº¦: {file_ops['throughput']['files_per_second']:.1f} æ–‡ä»¶/ç§’, {file_ops['throughput']['mb_per_second']:.1f} MB/ç§’")
            print()
        
        # æ€§èƒ½æ€»ç»“
        print("ğŸ¯ æ€§èƒ½æ€»ç»“:")
        if "offline_sync" in results:
            offline = results["offline_sync"]
            events_count = offline['events_count']
            markets_count = offline['markets_count']
            avg_time = offline['avg_time']
            
            print(f"   âš¡ ç¦»çº¿åŒæ­¥: {avg_time:.2f}ç§’ å¤„ç† {events_count}äº‹ä»¶ + {markets_count}å¸‚åœº")
            print(f"   ğŸ“ˆ å¤„ç†é€Ÿåº¦: {(events_count + markets_count) / avg_time:.1f} æ¡è®°å½•/ç§’")
            
            # é¢„ä¼°ä¸åŒæ•°æ®é‡çš„åŒæ­¥æ—¶é—´
            print(f"   ğŸ“Š é¢„ä¼°åŒæ­¥æ—¶é—´:")
            for scale, multiplier in [("å°è§„æ¨¡(100äº‹ä»¶)", 4), ("ä¸­è§„æ¨¡(500äº‹ä»¶)", 20), ("å¤§è§„æ¨¡(1000äº‹ä»¶)", 40)]:
                estimated_time = avg_time * multiplier
                print(f"      {scale}: ~{estimated_time:.1f}ç§’")
        
        print("="*80)
    
    def save_results(self, summary: Dict[str, Any], filename: str = None):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"sync_performance_{timestamp}.json"
        
        os.makedirs(os.path.join(self.data_dir, "performance"), exist_ok=True)
        filepath = os.path.join(self.data_dir, "performance", filename)
        
        with open(filepath, "w") as f:
            json.dump(summary, f, indent=2)
        
        print(f"ğŸ’¾ æ€§èƒ½æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filepath}")

def main():
    import argparse
    parser = argparse.ArgumentParser(description="PolymarketåŒæ­¥æ€§èƒ½æµ‹è¯•")
    parser.add_argument("--data-dir", default="./data", help="æ•°æ®ç›®å½•")
    parser.add_argument("--mode", choices=["offline", "api", "generation", "files", "full"], 
                       default="full", help="æµ‹è¯•æ¨¡å¼")
    parser.add_argument("--iterations", type=int, default=3, help="ç¦»çº¿æµ‹è¯•è¿­ä»£æ¬¡æ•°")
    parser.add_argument("--save", action="store_true", help="ä¿å­˜ç»“æœåˆ°æ–‡ä»¶")
    
    args = parser.parse_args()
    
    tester = SyncPerformanceTest(args.data_dir)
    
    if args.mode == "offline":
        result = tester.test_offline_sync(args.iterations)
        summary = {"results": {"offline_sync": result}}
    elif args.mode == "api":
        result = tester.test_api_sync()
        summary = {"results": {"api_sync": result}}
    elif args.mode == "generation":
        result = tester.test_data_generation()
        summary = {"results": {"data_generation": result}}
    elif args.mode == "files":
        result = tester.benchmark_file_operations()
        summary = {"results": {"file_operations": result}}
    else:
        summary = tester.run_full_benchmark()
    
    tester.print_results(summary)
    
    if args.save:
        tester.save_results(summary)

if __name__ == "__main__":
    main()