#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆPolymarketåŒæ­¥å™¨ - çœŸå®APIæ¨¡å¼
å‚ç…§example.pyçš„æ¨¡å¼ï¼Œæ·»åŠ æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
"""

import json
import os
import csv
import time
import argparse
import logging
from datetime import datetime
from typing import List, Dict, Any, Optional
import pandas as pd
import requests

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EnhancedPolymarketSync:
    def __init__(self, data_dir: str = "./data", batch_size: int = 100, max_retries: int = 3):
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.max_retries = max_retries
        
        # æ ‡å‡†è¯·æ±‚å¤´
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "en-US,en;q=0.9",
            "Referer": "https://polymarket.com/"
        }
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        for subdir in ["tag", "sync_logs", "reports", "analysis", "markets", "events"]:
            os.makedirs(os.path.join(data_dir, subdir), exist_ok=True)

    def count_csv_lines(self, filename: str) -> int:
        """è®¡ç®—CSVæ–‡ä»¶çš„è¡Œæ•°ï¼ˆä¸åŒ…æ‹¬æ ‡é¢˜è¡Œï¼‰"""
        if not os.path.exists(filename):
            return 0
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                return sum(1 for line in f) - 1  # å‡å»æ ‡é¢˜è¡Œ
        except Exception:
            return 0

    def make_api_request(self, url: str, params: Optional[Dict] = None, timeout: int = 30) -> Optional[Dict]:
        """
        å‘é€APIè¯·æ±‚ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†
        """
        for attempt in range(self.max_retries):
            try:
                response = requests.get(url, params=params, headers=self.headers, timeout=timeout)
                
                # å¤„ç†ä¸åŒçš„HTTPçŠ¶æ€ç 
                if response.status_code == 500:
                    logger.warning(f"æœåŠ¡å™¨é”™è¯¯ (500) - ç¬¬{attempt + 1}æ¬¡é‡è¯•ï¼Œç­‰å¾…5ç§’...")
                    time.sleep(5)
                    continue
                elif response.status_code == 429:
                    logger.warning(f"è¯·æ±‚é™åˆ¶ (429) - ç¬¬{attempt + 1}æ¬¡é‡è¯•ï¼Œç­‰å¾…10ç§’...")
                    time.sleep(10)
                    continue
                elif response.status_code == 404:
                    logger.warning(f"èµ„æºæœªæ‰¾åˆ° (404): {url}")
                    return None
                elif response.status_code != 200:
                    logger.error(f"APIé”™è¯¯ {response.status_code}: {response.text[:200]}")
                    if attempt < self.max_retries - 1:
                        logger.info(f"ç¬¬{attempt + 1}æ¬¡é‡è¯•ï¼Œç­‰å¾…3ç§’...")
                        time.sleep(3)
                        continue
                    else:
                        return None
                
                # å°è¯•è§£æJSON
                try:
                    return response.json()
                except json.JSONDecodeError as e:
                    logger.error(f"JSONè§£æå¤±è´¥: {e}")
                    if response.text.strip():
                        logger.error(f"å“åº”å†…å®¹: {response.text[:500]}")
                    return None
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"ç½‘ç»œé”™è¯¯ (ç¬¬{attempt + 1}æ¬¡): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(5)
                    continue
                else:
                    return None
            except Exception as e:
                logger.error(f"æ„å¤–é”™è¯¯ (ç¬¬{attempt + 1}æ¬¡): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(3)
                    continue
                else:
                    return None
        
        return None

    def update_markets(self, tag_id: Optional[int] = None, csv_filename: Optional[str] = None) -> Dict[str, Any]:
        """
        æ‰¹é‡è·å–å¸‚åœºæ•°æ®å¹¶ä¿å­˜åˆ°CSVï¼ŒåŸºäºå®Œæ•´çš„marketæ•°æ®ç»“æ„
        
        Args:
            tag_id: å¯é€‰çš„æ ‡ç­¾IDï¼Œç”¨äºç­›é€‰ç‰¹å®šæ ‡ç­¾çš„å¸‚åœº
            csv_filename: å¯é€‰çš„CSVæ–‡ä»¶åï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆå¸¦æ—¥æœŸçš„æ–‡ä»¶å
        """
        base_url = "https://gamma-api.polymarket.com/markets"
        
        # ç”Ÿæˆå¸¦æ—¥æœŸåç¼€çš„æ–‡ä»¶å
        if csv_filename is None:
            date_suffix = datetime.now().strftime("%Y-%m-%d")
            if tag_id is not None:
                csv_filename = f"markets_tag_{tag_id}_{date_suffix}.csv"
            else:
                csv_filename = f"markets_{date_suffix}.csv"
        
        full_path = os.path.join(self.data_dir, "markets", csv_filename)
        
        # ç²¾ç®€çš„CSVæ ‡é¢˜ - åªä¿ç•™æŒ‡å®šçš„æ ¸å¿ƒå­—æ®µ
        headers = [
            'id', 'question', 'slug', 'category', 'clobTokenIds', 'outcomes', 
            'outcomePrices', 'conditionId', 'active', 'closed', 'volumeNum', 
            'volume24hr', 'liquidity', 'liquidityNum', 'endDate', 
            'orderPriceMinTickSize', 'orderMinSize', 'resolutionSource', 
            'acceptingOrders', 'openInterest'
        ]
        
        # æ ¹æ®ç°æœ‰è®°å½•åŠ¨æ€è®¾ç½®åç§»é‡
        current_offset = self.count_csv_lines(full_path)
        file_exists = os.path.exists(full_path) and current_offset > 0
        
        if file_exists:
            if tag_id is not None:
                logger.info(f"å‘ç° {current_offset} æ¡ç°æœ‰è®°å½• (æ ‡ç­¾ID: {tag_id})ï¼Œä»åç§»é‡ {current_offset} ç»§ç»­")
            else:
                logger.info(f"å‘ç° {current_offset} æ¡ç°æœ‰è®°å½•ï¼Œä»åç§»é‡ {current_offset} ç»§ç»­")
            mode = 'a'
        else:
            if tag_id is not None:
                logger.info(f"åˆ›å»ºæ–°çš„CSVæ–‡ä»¶ (æ ‡ç­¾ID: {tag_id}): {full_path}")
            else:
                logger.info(f"åˆ›å»ºæ–°çš„CSVæ–‡ä»¶: {full_path}")
            mode = 'w'
        
        total_fetched = 0
        start_time = datetime.now()
        
        with open(full_path, mode, newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            
            # åªæœ‰æ–°æ–‡ä»¶æ‰å†™å…¥æ ‡é¢˜
            if mode == 'w':
                writer.writerow(headers)
            
            while True:
                if tag_id is not None:
                    logger.info(f"è·å–åç§»é‡ {current_offset} çš„æ‰¹æ¬¡æ•°æ® (æ ‡ç­¾ID: {tag_id})...")
                else:
                    logger.info(f"è·å–åç§»é‡ {current_offset} çš„æ‰¹æ¬¡æ•°æ®...")
                
                params = {
                    'order': 'createdAt',
                    'closed': 'false',
                    'ascending': 'true',
                    'limit': self.batch_size,
                    'offset': current_offset
                }
                
                # æ·»åŠ tag_idæŸ¥è¯¢æ¡ä»¶
                if tag_id is not None:
                    params['tag_id'] = tag_id
                
                data = self.make_api_request(base_url, params)
                
                if not data:
                    logger.error(f"æ— æ³•è·å–åç§»é‡ {current_offset} çš„æ•°æ®")
                    break
                
                markets = data if isinstance(data, list) else data.get('data', [])
                
                if not markets:
                    logger.info(f"åœ¨åç§»é‡ {current_offset} æ²¡æœ‰æ‰¾åˆ°æ›´å¤šå¸‚åœºï¼Œå®Œæˆï¼")
                    break
                
                batch_count = 0
                
                for market in markets:
                    try:
                        # å¤„ç†JSONå­—æ®µ
                        def safe_json_field(field_value):
                            """å®‰å…¨å¤„ç†JSONå­—æ®µï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²"""
                            if isinstance(field_value, (list, dict)):
                                return json.dumps(field_value)
                            elif isinstance(field_value, str):
                                return field_value
                            else:
                                return str(field_value) if field_value is not None else ''
                        
                        # åˆ›å»ºç²¾ç®€çš„æ•°æ®è¡Œï¼ŒæŒ‰ç…§headersé¡ºåº
                        row = [
                            market.get('id', ''),
                            market.get('question', ''),
                            market.get('slug', ''),
                            market.get('category', ''),
                            safe_json_field(market.get('clobTokenIds', '')),
                            safe_json_field(market.get('outcomes', '')),
                            safe_json_field(market.get('outcomePrices', '')),
                            market.get('conditionId', ''),
                            market.get('active', ''),
                            market.get('closed', ''),
                            market.get('volumeNum', ''),
                            market.get('volume24hr', ''),
                            market.get('liquidity', ''),
                            market.get('liquidityNum', ''),
                            market.get('endDate', ''),
                            market.get('orderPriceMinTickSize', ''),
                            market.get('orderMinSize', ''),
                            market.get('resolutionSource', ''),
                            market.get('acceptingOrders', ''),
                            market.get('openInterest', '')
                        ]
                        
                        writer.writerow(row)
                        batch_count += 1
                        
                    except (ValueError, KeyError, json.JSONDecodeError) as e:
                        logger.error(f"å¤„ç†å¸‚åœº {market.get('id', 'unknown')} æ—¶å‡ºé”™: {e}")
                        continue
                
                total_fetched += batch_count
                current_offset += batch_count
                
                logger.info(f"å¤„ç†äº† {batch_count} ä¸ªå¸‚åœºã€‚æ€»æ–°å¢: {total_fetched}ã€‚ä¸‹ä¸€ä¸ªåç§»é‡: {current_offset}")
                
                # å¦‚æœè·å–çš„å¸‚åœºæ•°å°‘äºæ‰¹æ¬¡å¤§å°ï¼Œè¯´æ˜åˆ°è¾¾æœ«å°¾
                if len(markets) < self.batch_size:
                    logger.info(f"åªæ”¶åˆ° {len(markets)} ä¸ªå¸‚åœºï¼ˆå°‘äºæ‰¹æ¬¡å¤§å°ï¼‰ï¼Œå·²åˆ°è¾¾æœ«å°¾")
                    break
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        return {
            "total_fetched": total_fetched,
            "final_offset": current_offset,
            "duration_seconds": duration,
            "csv_file": full_path,
            "csv_filename": csv_filename,
            "tag_id": tag_id
        }

    def update_events(self, csv_filename: str = "events.csv") -> Dict[str, Any]:
        """
        æ‰¹é‡è·å–äº‹ä»¶æ•°æ®å¹¶ä¿å­˜åˆ°CSV
        """
        base_url = "https://gamma-api.polymarket.com/events"
        full_path = os.path.join(self.data_dir, "events", csv_filename)
        
        # CSVæ ‡é¢˜
        headers = [
            'id', 'slug', 'title', 'resolutionSource', 'startDate', 'endDate',
             'category', 'active', 'closed', 'volume', 'volumn24hr','volume1wk','liquidity',
             'liquidityAmm','liquidityClob', 'commnentCount','tags', 'ticker', 'image'
        ]
        
        # æ ¹æ®ç°æœ‰è®°å½•åŠ¨æ€è®¾ç½®åç§»é‡
        current_offset = self.count_csv_lines(full_path)
        file_exists = os.path.exists(full_path) and current_offset > 0
        
        if file_exists:
            logger.info(f"å‘ç° {current_offset} æ¡ç°æœ‰äº‹ä»¶è®°å½•ï¼Œä»åç§»é‡ {current_offset} ç»§ç»­")
            mode = 'a'
        else:
            logger.info(f"åˆ›å»ºæ–°çš„äº‹ä»¶CSVæ–‡ä»¶: {full_path}")
            mode = 'w'
        
        total_fetched = 0
        start_time = datetime.now()
        
        with open(full_path, mode, newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            
            # åªæœ‰æ–°æ–‡ä»¶æ‰å†™å…¥æ ‡é¢˜
            if mode == 'w':
                writer.writerow(headers)
            
            while True:
                logger.info(f"è·å–äº‹ä»¶åç§»é‡ {current_offset} çš„æ‰¹æ¬¡æ•°æ®...")
                
                params = {
                    'limit': self.batch_size,
                    'offset': current_offset
                }
                
                data = self.make_api_request(base_url, params)
                
                if not data:
                    logger.error(f"æ— æ³•è·å–åç§»é‡ {current_offset} çš„äº‹ä»¶æ•°æ®")
                    break
                
                events = data if isinstance(data, list) else data.get('data', [])
                
                if not events:
                    logger.info(f"åœ¨åç§»é‡ {current_offset} æ²¡æœ‰æ‰¾åˆ°æ›´å¤šäº‹ä»¶ï¼Œå®Œæˆï¼")
                    break
                
                batch_count = 0
                
                for event in events:
                    try:
                        # å¤„ç†æ ‡ç­¾
                        tags = event.get('tags', [])
                        tags_str = ','.join(tags) if isinstance(tags, list) else str(tags)
                        
                        row = [
                            event.get('id', ''),
                            event.get('slug', ''),
                            event.get('title', ''),
                            event.get('description', ''),
                            event.get('createdAt', ''),
                            event.get('startDate', ''),
                            event.get('endDate', ''),
                            event.get('volume', ''),
                            event.get('liquidity', ''),
                            event.get('active', ''),
                            event.get('closed', ''),
                            tags_str,
                            event.get('ticker', ''),
                            event.get('image', '')
                        ]
                        
                        writer.writerow(row)
                        batch_count += 1
                        
                    except Exception as e:
                        logger.error(f"å¤„ç†äº‹ä»¶ {event.get('id', 'unknown')} æ—¶å‡ºé”™: {e}")
                        continue
                
                total_fetched += batch_count
                current_offset += batch_count
                
                logger.info(f"å¤„ç†äº† {batch_count} ä¸ªäº‹ä»¶ã€‚æ€»æ–°å¢: {total_fetched}ã€‚ä¸‹ä¸€ä¸ªåç§»é‡: {current_offset}")
                
                # å¦‚æœè·å–çš„äº‹ä»¶æ•°å°‘äºæ‰¹æ¬¡å¤§å°ï¼Œè¯´æ˜åˆ°è¾¾æœ«å°¾
                if len(events) < self.batch_size:
                    logger.info(f"åªæ”¶åˆ° {len(events)} ä¸ªäº‹ä»¶ï¼ˆå°‘äºæ‰¹æ¬¡å¤§å°ï¼‰ï¼Œå·²åˆ°è¾¾æœ«å°¾")
                    break
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        return {
            "total_fetched": total_fetched,
            "final_offset": current_offset,
            "duration_seconds": duration,
            "csv_file": full_path
        }

    def update_markets_by_tag(self, tag_id: int, csv_filename: Optional[str] = None) -> Dict[str, Any]:
        """
        æ ¹æ®æ ‡ç­¾IDè·å–å¸‚åœºæ•°æ®çš„ä¾¿æ·æ–¹æ³•
        
        Args:
            tag_id: æ ‡ç­¾ID
            csv_filename: å¯é€‰çš„CSVæ–‡ä»¶å
            
        Returns:
            åŒæ­¥ç»“æœå­—å…¸
        """
        logger.info(f"å¼€å§‹åŒæ­¥æ ‡ç­¾ID {tag_id} çš„å¸‚åœºæ•°æ®...")
        return self.update_markets(tag_id=tag_id, csv_filename=csv_filename)

    def get_available_filename_formats(self) -> Dict[str, str]:
        """
        è·å–å¯ç”¨çš„æ–‡ä»¶åæ ¼å¼ç¤ºä¾‹
        
        Returns:
            æ–‡ä»¶åæ ¼å¼ç¤ºä¾‹å­—å…¸
        """
        date_suffix = datetime.now().strftime("%Y-%m-%d")
        return {
            "default": f"markets_{date_suffix}.csv",
            "with_tag": f"markets_tag_123_{date_suffix}.csv",
            "custom": "custom_markets_name.csv"
        }
        
    def test_market_csv_structure(self) -> bool:
        logger.info("æµ‹è¯•å¸‚åœºCSVç»“æ„...")
        
        # æ¨¡æ‹Ÿå¸‚åœºæ•°æ®
        sample_market = {
            "id": "test_market_123",
            "question": "Will this test pass?",
            "conditionId": "0x123abc",
            "slug": "test-market",
            "volume": "1000.50",
            "active": True,
            "closed": False,
            "outcomes": ["Yes", "No"],
            "clobTokenIds": ["token1", "token2"],
            "createdAt": "2023-11-07T05:31:56Z"
        }
        
        # æµ‹è¯•safe_json_fieldå‡½æ•°
        def safe_json_field(field_value):
            if isinstance(field_value, (list, dict)):
                return json.dumps(field_value)
            elif isinstance(field_value, str):
                return field_value
            else:
                return str(field_value) if field_value is not None else ''
        
        # æµ‹è¯•æ•°æ®å¤„ç†
        outcomes_json = safe_json_field(sample_market.get('outcomes'))
        tokens_json = safe_json_field(sample_market.get('clobTokenIds'))
        
        print(f"âœ… æµ‹è¯•ç»“æœ:")
        print(f"   Outcomes JSON: {outcomes_json}")
        print(f"   Tokens JSON: {tokens_json}")
        print(f"   å¸‚åœºID: {sample_market.get('id')}")
        print(f"   é—®é¢˜: {sample_market.get('question')}")
        
        logger.info("å¸‚åœºCSVç»“æ„æµ‹è¯•å®Œæˆ")
        return True

    def get_market_csv_headers(self) -> List[str]:
        """
        è·å–å¸‚åœºCSVè¡¨å¤´åˆ—è¡¨ï¼Œç”¨äºå¤–éƒ¨å¼•ç”¨
        """
        return [
            'id', 'question', 'slug', 'category', 'clobTokenIds', 'outcomes', 
            'outcomePrices', 'conditionId', 'active', 'closed', 'volumeNum', 
            'volume24hr', 'liquidity', 'liquidityNum', 'endDate', 
            'orderPriceMinTickSize', 'orderMinSize', 'resolutionSource', 
            'acceptingOrders', 'openInterest'
        ]

    def test_market_csv_structure(self) -> bool:
        """
        æµ‹è¯•æ–°çš„å¸‚åœºCSVç»“æ„
        """
        logger.info("æµ‹è¯•å¸‚åœºCSVç»“æ„...")
        
        # æ¨¡æ‹Ÿå¸‚åœºæ•°æ®
        sample_market = {
            "id": "test_market_123",
            "question": "Will this test pass?",
            "conditionId": "0x123abc",
            "slug": "test-market",
            "volume": "1000.50",
            "active": True,
            "closed": False,
            "outcomes": ["Yes", "No"],
            "clobTokenIds": ["token1", "token2"],
            "createdAt": "2023-11-07T05:31:56Z"
        }
        
        # æµ‹è¯•safe_json_fieldå‡½æ•°
        def safe_json_field(field_value):
            if isinstance(field_value, (list, dict)):
                return json.dumps(field_value)
            elif isinstance(field_value, str):
                return field_value
            else:
                return str(field_value) if field_value is not None else ''
        
        # æµ‹è¯•æ•°æ®å¤„ç†
        outcomes_json = safe_json_field(sample_market.get('outcomes'))
        tokens_json = safe_json_field(sample_market.get('clobTokenIds'))
        
        print(f"âœ… æµ‹è¯•ç»“æœ:")
        print(f"   Outcomes JSON: {outcomes_json}")
        print(f"   Tokens JSON: {tokens_json}")
        print(f"   å¸‚åœºID: {sample_market.get('id')}")
        print(f"   é—®é¢˜: {sample_market.get('question')}")
        
        logger.info("å¸‚åœºCSVç»“æ„æµ‹è¯•å®Œæˆ")
        return True

    def test_api_endpoints(self) -> Dict[str, Any]:
        """
        æµ‹è¯•APIç«¯ç‚¹è¿é€šæ€§
        """
        endpoints = {
            "markets": "https://gamma-api.polymarket.com/markets",
            "events": "https://gamma-api.polymarket.com/events"
        }
        
        results = {}
        
        for name, url in endpoints.items():
            try:
                response = requests.get(url, headers=self.headers, timeout=10)
                results[name] = {
                    "status_code": response.status_code,
                    "success": response.status_code == 200,
                    "content_type": response.headers.get("content-type", ""),
                    "content_length": len(response.text),
                    "has_json": False,
                    "data_type": None,
                    "error": None
                }
                
                if response.status_code == 200:
                    try:
                        data = response.json()
                        results[name]["has_json"] = True
                        results[name]["data_type"] = type(data).__name__
                        if isinstance(data, list):
                            results[name]["data_count"] = len(data)
                        elif isinstance(data, dict):
                            results[name]["data_keys"] = list(data.keys())
                    except json.JSONDecodeError:
                        results[name]["error"] = "Invalid JSON"
                else:
                    results[name]["error"] = f"HTTP {response.status_code}"
                    
            except Exception as e:
                results[name] = {
                    "success": False,
                    "error": str(e)
                }
        
        # æ‰“å°æµ‹è¯•ç»“æœ
        print("\n" + "="*60)
        print("ğŸ” APIç«¯ç‚¹æµ‹è¯•ç»“æœ")
        print("="*60)
        
        for name, result in results.items():
            status = "âœ…" if result.get("success") else "âŒ"
            print(f"{status} {name.upper()}: {endpoints[name]}")
            
            if result.get("success"):
                print(f"   çŠ¶æ€ç : {result['status_code']}")
                print(f"   å†…å®¹ç±»å‹: {result['content_type']}")
                print(f"   æ•°æ®ç±»å‹: {result.get('data_type', 'N/A')}")
                if 'data_count' in result:
                    print(f"   æ•°æ®æ•°é‡: {result['data_count']}")
                elif 'data_keys' in result:
                    print(f"   æ•°æ®é”®: {result['data_keys']}")
            else:
                print(f"   é”™è¯¯: {result.get('error', 'Unknown')}")
            print()
        
        return results

    def sync_all_data(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„æ•°æ®åŒæ­¥ï¼ŒåŒ…æ‹¬å¸‚åœºå’Œäº‹ä»¶
        """
        start_time = datetime.now()
        logger.info("å¼€å§‹å®Œæ•´æ•°æ®åŒæ­¥...")
        
        results = {
            "start_time": start_time.isoformat(),
            "markets": None,
            "events": None,
            "errors": []
        }
        
        try:
            # åŒæ­¥å¸‚åœºæ•°æ®
            logger.info("å¼€å§‹åŒæ­¥å¸‚åœºæ•°æ®...")
            markets_result = self.update_markets()
            results["markets"] = markets_result
            logger.info(f"å¸‚åœºåŒæ­¥å®Œæˆ: {markets_result['total_fetched']} æ¡è®°å½•")
            
        except Exception as e:
            error_msg = f"å¸‚åœºåŒæ­¥å¤±è´¥: {e}"
            logger.error(error_msg)
            results["errors"].append(error_msg)
        
        try:
            # åŒæ­¥äº‹ä»¶æ•°æ®
            logger.info("å¼€å§‹åŒæ­¥äº‹ä»¶æ•°æ®...")
            events_result = self.update_events()
            results["events"] = events_result
            logger.info(f"äº‹ä»¶åŒæ­¥å®Œæˆ: {events_result['total_fetched']} æ¡è®°å½•")
            
        except Exception as e:
            error_msg = f"äº‹ä»¶åŒæ­¥å¤±è´¥: {e}"
            logger.error(error_msg)
            results["errors"].append(error_msg)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        results.update({
            "end_time": end_time.isoformat(),
            "duration_seconds": duration,
            "success": len(results["errors"]) == 0
        })
        
        # ç”ŸæˆæŠ¥å‘Š
        self._save_sync_report(results)
        self._print_sync_summary(results)
        
        return results

    def _save_sync_report(self, results: Dict[str, Any]):
        """ä¿å­˜åŒæ­¥æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONæŠ¥å‘Š
        json_file = os.path.join(self.data_dir, "reports", f"sync_report_{timestamp}.json")
        with open(json_file, "w", encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"åŒæ­¥æŠ¥å‘Šå·²ä¿å­˜: {json_file}")

    def _print_sync_summary(self, results: Dict[str, Any]):
        """æ‰“å°åŒæ­¥æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ‰ Polymarketæ•°æ®åŒæ­¥å®Œæˆ")
        print("="*80)
        print(f"â±ï¸  æ€»è€—æ—¶: {results['duration_seconds']:.1f} ç§’")
        
        if results.get("markets"):
            markets = results["markets"]
            print(f"ğŸ’¹ å¸‚åœº: {markets['total_fetched']} æ¡æ–°è®°å½•")
            print(f"   æ–‡ä»¶: {markets['csv_file']}")
        
        if results.get("events"):
            events = results["events"]
            print(f"ğŸ“… äº‹ä»¶: {events['total_fetched']} æ¡æ–°è®°å½•")
            print(f"   æ–‡ä»¶: {events['csv_file']}")
        
        if results.get("errors"):
            print(f"âŒ é”™è¯¯: {len(results['errors'])} ä¸ª")
            for error in results["errors"]:
                print(f"   - {error}")
        
def main():
    parser = argparse.ArgumentParser(description="å¢å¼ºç‰ˆPolymarketåŒæ­¥å™¨")
    parser.add_argument("--data-dir", default="./data", help="æ•°æ®ç›®å½•")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•APIç«¯ç‚¹")
    parser.add_argument("--test-csv", action="store_true", help="æµ‹è¯•CSVç»“æ„")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ—¥å¿—")
    parser.add_argument("--mode", choices=["markets", "events", "all"], default="all", help="åŒæ­¥æ¨¡å¼")
    parser.add_argument("--batch-size", type=int, default=100, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--tag-id", type=int, help="æ ‡ç­¾IDï¼Œç”¨äºç­›é€‰ç‰¹å®šæ ‡ç­¾çš„å¸‚åœº")
    parser.add_argument("--filename", help="è‡ªå®šä¹‰CSVæ–‡ä»¶åï¼ˆä¸åŒ…å«è·¯å¾„ï¼‰")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    syncer = EnhancedPolymarketSync(data_dir=args.data_dir, batch_size=args.batch_size)
    
    if args.test:
        syncer.test_api_endpoints()
    elif args.test_csv:
        syncer.test_market_csv_structure()
        headers = syncer.get_market_csv_headers()
        print(f"\nğŸ“Š å¸‚åœºCSVåŒ…å« {len(headers)} ä¸ªå­—æ®µ:")
        for i, header in enumerate(headers, 1):
            print(f"  {i:2d}. {header}")
    elif args.mode == "markets":
        result = syncer.update_markets(tag_id=args.tag_id, csv_filename=args.filename)
        print(f"âœ… å¸‚åœºåŒæ­¥å®Œæˆ: {result['total_fetched']} æ¡è®°å½•")
        print(f"ğŸ“ æ–‡ä»¶: {result['csv_filename']}")
        if result['tag_id']:
            print(f"ğŸ·ï¸  æ ‡ç­¾ID: {result['tag_id']}")
    elif args.mode == "events":
        result = syncer.update_events()
        print(f"âœ… äº‹ä»¶åŒæ­¥å®Œæˆ: {result['total_fetched']} æ¡è®°å½•")
    else:
        syncer.sync_all_data()

if __name__ == "__main__":
    main()