#!/usr/bin/env python3
"""
Polymarket å¸‚åœºåŒæ­¥å™¨
åŒæ­¥æ‰€æœ‰æ´»è·ƒäº‹ä»¶ï¼ŒæŒ‰ç…§GET /tagsè¿›è¡Œåˆ†ç±»ï¼Œå¹¶æ ¹æ®tagåˆ†ç»„ä¿å­˜åœ¨/data/tagç›®å½•ä¸‹
"""

import os
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Optional, Set
from collections import defaultdict
import pandas as pd
from core.polymarket_market_client import PolymarketMarketClient

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('polymarket_sync.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class PolymarketSynchronizer:
    """Polymarketå¸‚åœºåŒæ­¥å™¨"""
    
    def __init__(self, 
                 base_url: str = "https://gamma-api.polymarket.com",
                 data_dir: str = "./data",
                 use_mock_data: bool = False):
        
        self.client = PolymarketMarketClient(base_url=base_url, save_data=False)
        self.data_dir = data_dir
        self.tag_dir = os.path.join(data_dir, "tag")
        self.use_mock_data = use_mock_data
        
        # åˆ›å»ºç›®å½•ç»“æ„
        self.ensure_directories()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.sync_stats = {
            'total_events': 0,
            'total_markets': 0,
            'total_tags': 0,
            'events_by_tag': defaultdict(int),
            'markets_by_tag': defaultdict(int),
            'sync_start_time': None,
            'sync_end_time': None
        }
    
    def ensure_directories(self):
        """ç¡®ä¿ç›®å½•ç»“æ„å­˜åœ¨"""
        directories = [
            self.data_dir,
            self.tag_dir,
            os.path.join(self.data_dir, "sync_logs"),
            os.path.join(self.data_dir, "reports")
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            logger.info(f"ç¡®ä¿ç›®å½•å­˜åœ¨: {directory}")
    
    def get_all_tags(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æ ‡ç­¾ - GET /tags"""
        logger.info("è·å–æ‰€æœ‰æ ‡ç­¾...")
        
        if self.use_mock_data:
            return self.get_mock_tags()
        
        try:
            # å°è¯•è·å–æ ‡ç­¾ç«¯ç‚¹
            url = f"{self.client.base_url}/tags"
            response = self.client.session.get(url, timeout=10)
            
            if response.status_code == 200:
                tags_data = response.json()
                logger.info(f"æˆåŠŸè·å– {len(tags_data)} ä¸ªæ ‡ç­¾")
                return tags_data
            else:
                logger.warning(f"è·å–æ ‡ç­¾å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return self.get_mock_tags()
                
        except Exception as e:
            logger.error(f"è·å–æ ‡ç­¾å¤±è´¥: {e}")
            return self.get_mock_tags()
    
    def get_mock_tags(self) -> List[Dict]:
        """è·å–æ¨¡æ‹Ÿæ ‡ç­¾æ•°æ®"""
        return [
            {"id": "politics", "name": "Politics", "slug": "politics", "description": "Political events and elections"},
            {"id": "crypto", "name": "Crypto", "slug": "crypto", "description": "Cryptocurrency and blockchain"},
            {"id": "sports", "name": "Sports", "slug": "sports", "description": "Sports events and competitions"},
            {"id": "economics", "name": "Economics", "slug": "economics", "description": "Economic indicators and markets"},
            {"id": "technology", "name": "Technology", "slug": "technology", "description": "Technology and innovation"},
            {"id": "entertainment", "name": "Entertainment", "slug": "entertainment", "description": "Entertainment and media"},
            {"id": "science", "name": "Science", "slug": "science", "description": "Scientific discoveries and research"},
            {"id": "weather", "name": "Weather", "slug": "weather", "description": "Weather and climate events"},
            {"id": "business", "name": "Business", "slug": "business", "description": "Business and corporate events"},
            {"id": "social", "name": "Social", "slug": "social", "description": "Social trends and phenomena"}
        ]
    
    def get_all_active_events(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æ´»è·ƒäº‹ä»¶"""
        logger.info("è·å–æ‰€æœ‰æ´»è·ƒäº‹ä»¶...")
        
        if self.use_mock_data:
            return self.get_mock_events()
        
        all_events = []
        offset = 0
        limit = 100
        
        while True:
            try:
                # è·å–ä¸€æ‰¹äº‹ä»¶
                events = self.client.get_events(
                    active=True, 
                    closed=False, 
                    limit=limit, 
                    offset=offset
                )
                
                if not events or len(events) == 0:
                    break
                
                all_events.extend(events)
                logger.info(f"å·²è·å– {len(all_events)} ä¸ªäº‹ä»¶...")
                
                # å¦‚æœè¿”å›çš„äº‹ä»¶æ•°é‡å°‘äºlimitï¼Œè¯´æ˜å·²ç»åˆ°æœ€åä¸€é¡µ
                if len(events) < limit:
                    break
                
                offset += limit
                time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                
            except Exception as e:
                logger.error(f"è·å–äº‹ä»¶å¤±è´¥ (offset={offset}): {e}")
                break
        
        logger.info(f"æ€»å…±è·å–åˆ° {len(all_events)} ä¸ªæ´»è·ƒäº‹ä»¶")
        return all_events
    
    def get_mock_events(self) -> List[Dict]:
        """è·å–æ¨¡æ‹Ÿäº‹ä»¶æ•°æ®"""
        return [
            {
                "id": "event_001",
                "slug": "2024-us-presidential-election",
                "title": "2024 US Presidential Election",
                "description": "Who will win the 2024 United States Presidential Election?",
                "active": True,
                "closed": False,
                "tags": ["politics", "elections", "usa"],
                "category": "Politics",
                "volume": 15000000,
                "liquidity": 2500000,
                "start_date": "2024-01-01T00:00:00Z",
                "end_date": "2024-11-05T23:59:59Z"
            },
            {
                "id": "event_002",
                "slug": "bitcoin-price-100k-2024",
                "title": "Bitcoin Price $100K in 2024",
                "description": "Will Bitcoin reach $100,000 by the end of 2024?",
                "active": True,
                "closed": False,
                "tags": ["crypto", "bitcoin", "price"],
                "category": "Crypto",
                "volume": 8000000,
                "liquidity": 1200000,
                "start_date": "2024-01-01T00:00:00Z",
                "end_date": "2024-12-31T23:59:59Z"
            },
            {
                "id": "event_003",
                "slug": "super-bowl-2025-winner",
                "title": "Super Bowl 2025 Winner",
                "description": "Which team will win Super Bowl LIX?",
                "active": True,
                "closed": False,
                "tags": ["sports", "nfl", "football"],
                "category": "Sports",
                "volume": 5000000,
                "liquidity": 800000,
                "start_date": "2024-09-01T00:00:00Z",
                "end_date": "2025-02-09T23:59:59Z"
            },
            {
                "id": "event_004",
                "slug": "fed-rate-cut-2024",
                "title": "Federal Reserve Rate Cut 2024",
                "description": "Will the Federal Reserve cut interest rates in 2024?",
                "active": True,
                "closed": False,
                "tags": ["economics", "fed", "interest-rates"],
                "category": "Economics",
                "volume": 3000000,
                "liquidity": 500000,
                "start_date": "2024-01-01T00:00:00Z",
                "end_date": "2024-12-31T23:59:59Z"
            },
            {
                "id": "event_005",
                "slug": "ai-breakthrough-2024",
                "title": "Major AI Breakthrough 2024",
                "description": "Will there be a major AI breakthrough announced in 2024?",
                "active": True,
                "closed": False,
                "tags": ["technology", "ai", "innovation"],
                "category": "Technology",
                "volume": 2000000,
                "liquidity": 300000,
                "start_date": "2024-01-01T00:00:00Z",
                "end_date": "2024-12-31T23:59:59Z"
            }
        ]
    
    def get_markets_for_events(self, events: List[Dict]) -> Dict[str, List[Dict]]:
        """è·å–äº‹ä»¶å¯¹åº”çš„å¸‚åœº"""
        logger.info("è·å–äº‹ä»¶å¯¹åº”çš„å¸‚åœº...")
        
        event_markets = {}
        
        for i, event in enumerate(events):
            event_slug = event.get('slug')
            if not event_slug:
                continue
            
            try:
                # è·å–äº‹ä»¶çš„å¸‚åœº
                markets = self.client.get_event_markets(event_slug)
                if markets:
                    event_markets[event_slug] = markets
                    logger.info(f"äº‹ä»¶ {event_slug} æœ‰ {len(markets)} ä¸ªå¸‚åœº")
                else:
                    event_markets[event_slug] = []
                
                # è¿›åº¦æ˜¾ç¤º
                if (i + 1) % 10 == 0:
                    logger.info(f"å·²å¤„ç† {i + 1}/{len(events)} ä¸ªäº‹ä»¶")
                
                time.sleep(0.2)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                
            except Exception as e:
                logger.error(f"è·å–äº‹ä»¶ {event_slug} çš„å¸‚åœºå¤±è´¥: {e}")
                event_markets[event_slug] = []
        
        total_markets = sum(len(markets) for markets in event_markets.values())
        logger.info(f"æ€»å…±è·å–åˆ° {total_markets} ä¸ªå¸‚åœº")
        
        return event_markets
    
    def categorize_by_tags(self, events: List[Dict], event_markets: Dict[str, List[Dict]]) -> Dict[str, Dict]:
        """æŒ‰æ ‡ç­¾åˆ†ç±»äº‹ä»¶å’Œå¸‚åœº"""
        logger.info("æŒ‰æ ‡ç­¾åˆ†ç±»äº‹ä»¶å’Œå¸‚åœº...")
        
        tag_data = defaultdict(lambda: {
            'events': [],
            'markets': [],
            'tag_info': None
        })
        
        # å¤„ç†æ¯ä¸ªäº‹ä»¶
        for event in events:
            event_tags = event.get('tags', [])
            event_slug = event.get('slug', '')
            
            # ç¡®ä¿tagsæ˜¯åˆ—è¡¨æ ¼å¼
            if isinstance(event_tags, str):
                event_tags = [event_tags]
            elif not isinstance(event_tags, list):
                event_tags = []
            
            # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œå½’ç±»åˆ°"uncategorized"
            if not event_tags:
                event_tags = ['uncategorized']
            
            # ä¸ºæ¯ä¸ªæ ‡ç­¾æ·»åŠ äº‹ä»¶
            for tag in event_tags:
                tag_slug = str(tag).lower().replace(' ', '-')
                tag_data[tag_slug]['events'].append(event)
                
                # æ·»åŠ å¯¹åº”çš„å¸‚åœº
                markets = event_markets.get(event_slug, [])
                tag_data[tag_slug]['markets'].extend(markets)
        
        logger.info(f"æŒ‰ {len(tag_data)} ä¸ªæ ‡ç­¾åˆ†ç±»å®Œæˆ")
        return dict(tag_data)
    
    def save_tag_data(self, tag_slug: str, tag_data: Dict):
        """ä¿å­˜å•ä¸ªæ ‡ç­¾çš„æ•°æ®"""
        tag_directory = os.path.join(self.tag_dir, tag_slug)
        os.makedirs(tag_directory, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜äº‹ä»¶æ•°æ®
        events = tag_data['events']
        if events:
            events_df = pd.DataFrame([
                {
                    'id': event.get('id', ''),
                    'slug': event.get('slug', ''),
                    'title': event.get('title', ''),
                    'description': event.get('description', ''),
                    'active': event.get('active', False),
                    'closed': event.get('closed', False),
                    'category': event.get('category', ''),
                    'tags': ','.join([str(t) for t in event.get('tags', [])]),
                    'volume': event.get('volume', 0),
                    'liquidity': event.get('liquidity', 0),
                    'start_date': event.get('start_date', ''),
                    'end_date': event.get('end_date', ''),
                    'sync_timestamp': datetime.now().isoformat()
                }
                for event in events
            ])
            
            events_file = os.path.join(tag_directory, f"events_{timestamp}.csv")
            events_df.to_csv(events_file, index=False, encoding='utf-8')
            logger.info(f"ä¿å­˜ {len(events)} ä¸ªäº‹ä»¶åˆ°: {events_file}")
        
        # ä¿å­˜å¸‚åœºæ•°æ®
        markets = tag_data['markets']
        if markets:
            markets_df = pd.DataFrame([
                {
                    'id': market.get('id', ''),
                    'slug': market.get('slug', ''),
                    'question': market.get('question', ''),
                    'description': market.get('description', ''),
                    'event_slug': market.get('event_slug', ''),
                    'active': market.get('active', False),
                    'closed': market.get('closed', False),
                    'category': market.get('category', ''),
                    'volume': market.get('volume', 0),
                    'liquidity': market.get('liquidity', 0),
                    'end_date_iso': market.get('end_date_iso', ''),
                    'sync_timestamp': datetime.now().isoformat()
                }
                for market in markets
            ])
            
            markets_file = os.path.join(tag_directory, f"markets_{timestamp}.csv")
            markets_df.to_csv(markets_file, index=False, encoding='utf-8')
            logger.info(f"ä¿å­˜ {len(markets)} ä¸ªå¸‚åœºåˆ°: {markets_file}")
        
        # ä¿å­˜æ ‡ç­¾æ‘˜è¦
        summary = {
            'tag_slug': tag_slug,
            'events_count': len(events),
            'markets_count': len(markets),
            'total_volume': sum(event.get('volume', 0) for event in events),
            'total_liquidity': sum(event.get('liquidity', 0) for event in events),
            'sync_timestamp': datetime.now().isoformat(),
            'top_events': [
                {
                    'title': event.get('title', ''),
                    'volume': event.get('volume', 0)
                }
                for event in sorted(events, key=lambda x: x.get('volume', 0), reverse=True)[:5]
            ]
        }
        
        summary_file = os.path.join(tag_directory, f"summary_{timestamp}.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ä¿å­˜æ ‡ç­¾æ‘˜è¦åˆ°: {summary_file}")
        
        return {
            'events_file': events_file if events else None,
            'markets_file': markets_file if markets else None,
            'summary_file': summary_file
        }
    
    def generate_sync_report(self, categorized_data: Dict[str, Dict]) -> str:
        """ç”ŸæˆåŒæ­¥æŠ¥å‘Š"""
        report_data = {
            'sync_info': {
                'start_time': self.sync_stats['sync_start_time'],
                'end_time': self.sync_stats['sync_end_time'],
                'duration_seconds': (
                    datetime.fromisoformat(self.sync_stats['sync_end_time']) - 
                    datetime.fromisoformat(self.sync_stats['sync_start_time'])
                ).total_seconds() if self.sync_stats['sync_end_time'] else 0,
                'total_events': self.sync_stats['total_events'],
                'total_markets': self.sync_stats['total_markets'],
                'total_tags': len(categorized_data)
            },
            'tag_statistics': {},
            'top_tags_by_events': [],
            'top_tags_by_volume': []
        }
        
        # ç»Ÿè®¡æ¯ä¸ªæ ‡ç­¾
        tag_stats = []
        for tag_slug, tag_data in categorized_data.items():
            events = tag_data['events']
            markets = tag_data['markets']
            
            total_volume = sum(event.get('volume', 0) for event in events)
            total_liquidity = sum(event.get('liquidity', 0) for event in events)
            
            stat = {
                'tag': tag_slug,
                'events_count': len(events),
                'markets_count': len(markets),
                'total_volume': total_volume,
                'total_liquidity': total_liquidity
            }
            
            tag_stats.append(stat)
            report_data['tag_statistics'][tag_slug] = stat
        
        # æ’åºç»Ÿè®¡
        report_data['top_tags_by_events'] = sorted(
            tag_stats, key=lambda x: x['events_count'], reverse=True
        )[:10]
        
        report_data['top_tags_by_volume'] = sorted(
            tag_stats, key=lambda x: x['total_volume'], reverse=True
        )[:10]
        
        # ä¿å­˜æŠ¥å‘Š
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = os.path.join(self.data_dir, "reports", f"sync_report_{timestamp}.json")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"åŒæ­¥æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        text_report = self.generate_text_report(report_data)
        text_report_file = os.path.join(self.data_dir, "reports", f"sync_report_{timestamp}.txt")
        
        with open(text_report_file, 'w', encoding='utf-8') as f:
            f.write(text_report)
        
        logger.info(f"æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜åˆ°: {text_report_file}")
        
        return text_report
    
    def generate_text_report(self, report_data: Dict) -> str:
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„æŠ¥å‘Š"""
        sync_info = report_data['sync_info']
        
        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    Polymarket åŒæ­¥æŠ¥å‘Š                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ“Š åŒæ­¥ç»Ÿè®¡                                                  â•‘
â•‘   å¼€å§‹æ—¶é—´: {sync_info['start_time']}                        â•‘
â•‘   ç»“æŸæ—¶é—´: {sync_info['end_time']}                          â•‘
â•‘   è€—æ—¶: {sync_info['duration_seconds']:.1f} ç§’               â•‘
â•‘   æ€»äº‹ä»¶æ•°: {sync_info['total_events']:,}                   â•‘
â•‘   æ€»å¸‚åœºæ•°: {sync_info['total_markets']:,}                  â•‘
â•‘   æ ‡ç­¾æ•°é‡: {sync_info['total_tags']}                       â•‘
â•‘                                                              â•‘
â•‘ ğŸ·ï¸  çƒ­é—¨æ ‡ç­¾ (æŒ‰äº‹ä»¶æ•°)                                      â•‘"""
        
        for i, tag_stat in enumerate(report_data['top_tags_by_events'][:5], 1):
            report += f"""
â•‘   {i}. {tag_stat['tag']}: {tag_stat['events_count']} ä¸ªäº‹ä»¶   â•‘"""
        
        report += f"""
â•‘                                                              â•‘
â•‘ ğŸ’° çƒ­é—¨æ ‡ç­¾ (æŒ‰äº¤æ˜“é‡)                                       â•‘"""
        
        for i, tag_stat in enumerate(report_data['top_tags_by_volume'][:5], 1):
            volume_str = f"${tag_stat['total_volume']:,}"
            report += f"""
â•‘   {i}. {tag_stat['tag']}: {volume_str}                       â•‘"""
        
        report += f"""
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        
        return report.strip()
    
    def sync_all_markets(self) -> str:
        """åŒæ­¥æ‰€æœ‰å¸‚åœºçš„ä¸»æ–¹æ³•"""
        logger.info("å¼€å§‹åŒæ­¥Polymarketæ‰€æœ‰æ´»è·ƒå¸‚åœº...")
        self.sync_stats['sync_start_time'] = datetime.now().isoformat()
        
        try:
            # 1. è·å–æ‰€æœ‰æ ‡ç­¾
            tags = self.get_all_tags()
            self.sync_stats['total_tags'] = len(tags)
            
            # 2. è·å–æ‰€æœ‰æ´»è·ƒäº‹ä»¶
            events = self.get_all_active_events()
            self.sync_stats['total_events'] = len(events)
            
            if not events:
                logger.warning("æ²¡æœ‰è·å–åˆ°æ´»è·ƒäº‹ä»¶")
                return "åŒæ­¥å¤±è´¥: æ²¡æœ‰è·å–åˆ°æ´»è·ƒäº‹ä»¶"
            
            # 3. è·å–äº‹ä»¶å¯¹åº”çš„å¸‚åœº
            event_markets = self.get_markets_for_events(events)
            total_markets = sum(len(markets) for markets in event_markets.values())
            self.sync_stats['total_markets'] = total_markets
            
            # 4. æŒ‰æ ‡ç­¾åˆ†ç±»
            categorized_data = self.categorize_by_tags(events, event_markets)
            
            # 5. ä¿å­˜åˆ†ç±»æ•°æ®
            logger.info("ä¿å­˜åˆ†ç±»æ•°æ®åˆ°å„ä¸ªæ ‡ç­¾ç›®å½•...")
            saved_files = {}
            
            for tag_slug, tag_data in categorized_data.items():
                try:
                    files = self.save_tag_data(tag_slug, tag_data)
                    saved_files[tag_slug] = files
                    
                    # æ›´æ–°ç»Ÿè®¡
                    self.sync_stats['events_by_tag'][tag_slug] = len(tag_data['events'])
                    self.sync_stats['markets_by_tag'][tag_slug] = len(tag_data['markets'])
                    
                except Exception as e:
                    logger.error(f"ä¿å­˜æ ‡ç­¾ {tag_slug} æ•°æ®å¤±è´¥: {e}")
            
            # 6. ç”ŸæˆåŒæ­¥æŠ¥å‘Š
            self.sync_stats['sync_end_time'] = datetime.now().isoformat()
            report = self.generate_sync_report(categorized_data)
            
            logger.info("åŒæ­¥å®Œæˆ!")
            print(report)
            
            return report
            
        except Exception as e:
            logger.error(f"åŒæ­¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            self.sync_stats['sync_end_time'] = datetime.now().isoformat()
            return f"åŒæ­¥å¤±è´¥: {e}"
    
    def cleanup_old_files(self, days: int = 7):
        """æ¸…ç†æ—§çš„åŒæ­¥æ–‡ä»¶"""
        logger.info(f"æ¸…ç† {days} å¤©å‰çš„æ—§æ–‡ä»¶...")
        
        cutoff_time = datetime.now().timestamp() - (days * 24 * 3600)
        cleaned_count = 0
        
        # æ¸…ç†æ ‡ç­¾ç›®å½•ä¸‹çš„æ—§æ–‡ä»¶
        for tag_dir in os.listdir(self.tag_dir):
            tag_path = os.path.join(self.tag_dir, tag_dir)
            if os.path.isdir(tag_path):
                for filename in os.listdir(tag_path):
                    file_path = os.path.join(tag_path, filename)
                    if os.path.isfile(file_path) and os.path.getmtime(file_path) < cutoff_time:
                        os.remove(file_path)
                        cleaned_count += 1
        
        # æ¸…ç†æŠ¥å‘Šç›®å½•ä¸‹çš„æ—§æ–‡ä»¶
        reports_dir = os.path.join(self.data_dir, "reports")
        if os.path.exists(reports_dir):
            for filename in os.listdir(reports_dir):
                file_path = os.path.join(reports_dir, filename)
                if os.path.isfile(file_path) and os.path.getmtime(file_path) < cutoff_time:
                    os.remove(file_path)
                    cleaned_count += 1
        
        logger.info(f"æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleaned_count} ä¸ªæ—§æ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Polymarketå¸‚åœºåŒæ­¥å™¨')
    parser.add_argument('--mock', action='store_true', help='ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®')
    parser.add_argument('--cleanup', type=int, help='æ¸…ç†Nå¤©å‰çš„æ—§æ–‡ä»¶')
    parser.add_argument('--data-dir', default='./data', help='æ•°æ®ç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåŒæ­¥å™¨
    synchronizer = PolymarketSynchronizer(
        data_dir=args.data_dir,
        use_mock_data=args.mock
    )
    
    if args.cleanup:
        # æ¸…ç†æ—§æ–‡ä»¶
        synchronizer.cleanup_old_files(days=args.cleanup)
    else:
        # æ‰§è¡ŒåŒæ­¥
        if args.mock:
            print("ğŸ”§ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡ŒåŒæ­¥æ¼”ç¤º")
        
        report = synchronizer.sync_all_markets()
        
        print(f"\nğŸ“ æ•°æ®å·²ä¿å­˜åˆ°ç›®å½•ç»“æ„:")
        print(f"  {args.data_dir}/tag/[tag_name]/events_*.csv")
        print(f"  {args.data_dir}/tag/[tag_name]/markets_*.csv")
        print(f"  {args.data_dir}/tag/[tag_name]/summary_*.json")
        print(f"  {args.data_dir}/reports/sync_report_*.json")

if __name__ == "__main__":
    main()