#!/usr/bin/env python3
"""
Polymarket åŒæ­¥è°ƒåº¦å™¨
è‡ªåŠ¨åŒ–å®šæ—¶åŒæ­¥ï¼Œæ”¯æŒå¤šç§è°ƒåº¦ç­–ç•¥
"""

import os
import time
import json
import schedule
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Callable
import logging
from sync.polymarket_sync import PolymarketSynchronizer
from sync.sync_monitor import SyncMonitor

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SyncScheduler:
    """åŒæ­¥è°ƒåº¦å™¨"""
    
    def __init__(self, config_file: str = "sync_config.json"):
        self.config_file = config_file
        self.config = self.load_config()
        self.synchronizer = PolymarketSynchronizer(
            base_url=self.config['sync_settings']['base_url'],
            data_dir=self.config['sync_settings']['data_dir'],
            use_mock_data=self.config['sync_settings']['use_mock_data']
        )
        self.monitor = SyncMonitor(data_dir=self.config['sync_settings']['data_dir'])
        
        self.is_running = False
        self.scheduler_thread = None
        self.last_sync_result = None
        self.sync_history = []
        
        # å›è°ƒå‡½æ•°
        self.on_sync_success: Optional[Callable] = None
        self.on_sync_failure: Optional[Callable] = None
        self.on_schedule_start: Optional[Callable] = None
        self.on_schedule_stop: Optional[Callable] = None
    
    def load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.warning(f"é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self.get_default_config()
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return self.get_default_config()
    
    def get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "sync_settings": {
                "base_url": "https://gamma-api.polymarket.com",
                "data_dir": "./data",
                "use_mock_data": True,
                "batch_size": 100,
                "request_delay": 0.5,
                "timeout": 10,
                "max_retries": 3
            },
            "sync_schedule": {
                "enabled": True,
                "interval_hours": 6,
                "specific_times": ["06:00", "12:00", "18:00", "00:00"],
                "timezone": "UTC"
            },
            "notification_settings": {
                "email_alerts": False,
                "slack_webhook": "",
                "discord_webhook": "",
                "alert_on_sync_failure": True,
                "alert_on_large_changes": True,
                "change_threshold_percent": 20
            }
        }
    
    def setup_schedule(self):
        """è®¾ç½®è°ƒåº¦ä»»åŠ¡"""
        schedule.clear()  # æ¸…é™¤ç°æœ‰ä»»åŠ¡
        
        schedule_config = self.config.get('sync_schedule', {})
        
        if not schedule_config.get('enabled', False):
            logger.info("è°ƒåº¦åŠŸèƒ½å·²ç¦ç”¨")
            return
        
        # æŒ‰é—´éš”è°ƒåº¦
        interval_hours = schedule_config.get('interval_hours')
        if interval_hours:
            schedule.every(interval_hours).hours.do(self.run_sync_job)
            logger.info(f"è®¾ç½®é—´éš”è°ƒåº¦: æ¯ {interval_hours} å°æ—¶æ‰§è¡Œä¸€æ¬¡")
        
        # æŒ‰ç‰¹å®šæ—¶é—´è°ƒåº¦
        specific_times = schedule_config.get('specific_times', [])
        for time_str in specific_times:
            schedule.every().day.at(time_str).do(self.run_sync_job)
            logger.info(f"è®¾ç½®å®šæ—¶è°ƒåº¦: æ¯å¤© {time_str} æ‰§è¡Œ")
        
        logger.info(f"è°ƒåº¦è®¾ç½®å®Œæˆï¼Œå…± {len(schedule.jobs)} ä¸ªä»»åŠ¡")
    
    def run_sync_job(self):
        """æ‰§è¡ŒåŒæ­¥ä»»åŠ¡"""
        job_start_time = datetime.now()
        logger.info("å¼€å§‹æ‰§è¡Œè°ƒåº¦åŒæ­¥ä»»åŠ¡...")
        
        try:
            # æ£€æŸ¥æ•°æ®è´¨é‡
            quality_before = self.monitor.check_data_quality()
            
            # æ‰§è¡ŒåŒæ­¥
            sync_result = self.synchronizer.sync_all_markets()
            
            # æ£€æŸ¥åŒæ­¥åçš„æ•°æ®è´¨é‡
            quality_after = self.monitor.check_data_quality()
            
            # è®°å½•åŒæ­¥å†å²
            sync_record = {
                'timestamp': job_start_time.isoformat(),
                'duration_seconds': (datetime.now() - job_start_time).total_seconds(),
                'success': True,
                'quality_before': quality_before['quality_score'],
                'quality_after': quality_after['quality_score'],
                'events_synced': self.synchronizer.sync_stats.get('total_events', 0),
                'markets_synced': self.synchronizer.sync_stats.get('total_markets', 0),
                'tags_processed': self.synchronizer.sync_stats.get('total_tags', 0)
            }
            
            self.sync_history.append(sync_record)
            self.last_sync_result = sync_record
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤§å¹…å˜åŒ–
            self.check_for_significant_changes(quality_before, quality_after)
            
            # æˆåŠŸå›è°ƒ
            if self.on_sync_success:
                self.on_sync_success(sync_record)
            
            logger.info(f"è°ƒåº¦åŒæ­¥ä»»åŠ¡å®Œæˆï¼Œè€—æ—¶: {sync_record['duration_seconds']:.1f} ç§’")
            
        except Exception as e:
            # è®°å½•å¤±è´¥
            sync_record = {
                'timestamp': job_start_time.isoformat(),
                'duration_seconds': (datetime.now() - job_start_time).total_seconds(),
                'success': False,
                'error': str(e),
                'events_synced': 0,
                'markets_synced': 0,
                'tags_processed': 0
            }
            
            self.sync_history.append(sync_record)
            self.last_sync_result = sync_record
            
            # å¤±è´¥å›è°ƒ
            if self.on_sync_failure:
                self.on_sync_failure(sync_record)
            
            logger.error(f"è°ƒåº¦åŒæ­¥ä»»åŠ¡å¤±è´¥: {e}")
            
            # å‘é€å¤±è´¥é€šçŸ¥
            self.send_failure_notification(sync_record)
    
    def check_for_significant_changes(self, quality_before: Dict, quality_after: Dict):
        """æ£€æŸ¥æ˜¯å¦æœ‰æ˜¾è‘—å˜åŒ–"""
        threshold = self.config.get('notification_settings', {}).get('change_threshold_percent', 20)
        
        # æ£€æŸ¥è´¨é‡åˆ†æ•°å˜åŒ–
        quality_change = abs(quality_after['quality_score'] - quality_before['quality_score'])
        
        if quality_change > threshold:
            logger.warning(f"æ•°æ®è´¨é‡å‘ç”Ÿæ˜¾è‘—å˜åŒ–: {quality_before['quality_score']} -> {quality_after['quality_score']}")
            self.send_change_notification(quality_before, quality_after)
    
    def send_failure_notification(self, sync_record: Dict):
        """å‘é€å¤±è´¥é€šçŸ¥"""
        if not self.config.get('notification_settings', {}).get('alert_on_sync_failure', False):
            return
        
        message = f"""
ğŸš¨ Polymarket åŒæ­¥å¤±è´¥

æ—¶é—´: {sync_record['timestamp']}
é”™è¯¯: {sync_record.get('error', 'Unknown error')}
æŒç»­æ—¶é—´: {sync_record['duration_seconds']:.1f} ç§’

è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†ä¿¡æ¯ã€‚
        """.strip()
        
        self.send_notification("åŒæ­¥å¤±è´¥è­¦æŠ¥", message)
    
    def send_change_notification(self, quality_before: Dict, quality_after: Dict):
        """å‘é€å˜åŒ–é€šçŸ¥"""
        if not self.config.get('notification_settings', {}).get('alert_on_large_changes', False):
            return
        
        message = f"""
ğŸ“Š Polymarket æ•°æ®è´¨é‡å˜åŒ–

åŒæ­¥å‰è´¨é‡åˆ†æ•°: {quality_before['quality_score']}/100
åŒæ­¥åè´¨é‡åˆ†æ•°: {quality_after['quality_score']}/100

å˜åŒ–è¯¦æƒ…:
- é—®é¢˜æ•°é‡: {len(quality_before.get('issues', []))} -> {len(quality_after.get('issues', []))}
- è­¦å‘Šæ•°é‡: {len(quality_before.get('warnings', []))} -> {len(quality_after.get('warnings', []))}
        """.strip()
        
        self.send_notification("æ•°æ®è´¨é‡å˜åŒ–", message)
    
    def send_notification(self, title: str, message: str):
        """å‘é€é€šçŸ¥"""
        notification_config = self.config.get('notification_settings', {})
        
        # Slack é€šçŸ¥
        slack_webhook = notification_config.get('slack_webhook')
        if slack_webhook:
            self.send_slack_notification(slack_webhook, title, message)
        
        # Discord é€šçŸ¥
        discord_webhook = notification_config.get('discord_webhook')
        if discord_webhook:
            self.send_discord_notification(discord_webhook, title, message)
        
        # é‚®ä»¶é€šçŸ¥
        if notification_config.get('email_alerts', False):
            self.send_email_notification(title, message)
    
    def send_slack_notification(self, webhook_url: str, title: str, message: str):
        """å‘é€ Slack é€šçŸ¥"""
        try:
            import requests
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "application/json, text/plain, */*",
                "Accept-Language": "en-US,en;q=0.9",
                "Referer": "https://polymarket.com/"
            }
            
            payload = {
                "text": f"*{title}*\n```{message}```"
            }
            
            response = requests.post(webhook_url, json=payload, headers=headers, timeout=10)
            if response.status_code == 200:
                logger.info("Slack é€šçŸ¥å‘é€æˆåŠŸ")
            else:
                logger.error(f"Slack é€šçŸ¥å‘é€å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            logger.error(f"å‘é€ Slack é€šçŸ¥å¤±è´¥: {e}")
    
    def send_discord_notification(self, webhook_url: str, title: str, message: str):
        """å‘é€ Discord é€šçŸ¥"""
        try:
            import requests
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "application/json, text/plain, */*",
                "Accept-Language": "en-US,en;q=0.9",
                "Referer": "https://polymarket.com/"
            }
            
            payload = {
                "content": f"**{title}**\n```{message}```"
            }
            
            response = requests.post(webhook_url, json=payload, headers=headers, timeout=10)
            if response.status_code == 204:
                logger.info("Discord é€šçŸ¥å‘é€æˆåŠŸ")
            else:
                logger.error(f"Discord é€šçŸ¥å‘é€å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            logger.error(f"å‘é€ Discord é€šçŸ¥å¤±è´¥: {e}")
    
    def send_email_notification(self, title: str, message: str):
        """å‘é€é‚®ä»¶é€šçŸ¥"""
        # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶å‘é€åŠŸèƒ½
        logger.info(f"é‚®ä»¶é€šçŸ¥: {title}")
        logger.info(message)
    
    def start_scheduler(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        if self.is_running:
            logger.warning("è°ƒåº¦å™¨å·²åœ¨è¿è¡Œ")
            return
        
        self.setup_schedule()
        self.is_running = True
        
        def run_scheduler():
            logger.info("è°ƒåº¦å™¨å·²å¯åŠ¨")
            if self.on_schedule_start:
                self.on_schedule_start()
            
            while self.is_running:
                schedule.run_pending()
                time.sleep(1)
            
            logger.info("è°ƒåº¦å™¨å·²åœæ­¢")
            if self.on_schedule_stop:
                self.on_schedule_stop()
        
        self.scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        self.scheduler_thread.start()
        
        logger.info("è°ƒåº¦å™¨çº¿ç¨‹å·²å¯åŠ¨")
    
    def stop_scheduler(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        if not self.is_running:
            logger.warning("è°ƒåº¦å™¨æœªåœ¨è¿è¡Œ")
            return
        
        self.is_running = False
        
        if self.scheduler_thread:
            self.scheduler_thread.join(timeout=5)
        
        schedule.clear()
        logger.info("è°ƒåº¦å™¨å·²åœæ­¢")
    
    def get_schedule_status(self) -> Dict:
        """è·å–è°ƒåº¦çŠ¶æ€"""
        return {
            'is_running': self.is_running,
            'jobs_count': len(schedule.jobs),
            'jobs': [
                {
                    'job': str(job.job_func),
                    'next_run': job.next_run.isoformat() if job.next_run else None,
                    'interval': str(job.interval) if hasattr(job, 'interval') else None
                }
                for job in schedule.jobs
            ],
            'last_sync_result': self.last_sync_result,
            'sync_history_count': len(self.sync_history)
        }
    
    def get_sync_statistics(self) -> Dict:
        """è·å–åŒæ­¥ç»Ÿè®¡"""
        if not self.sync_history:
            return {'message': 'æš‚æ— åŒæ­¥å†å²'}
        
        successful_syncs = [s for s in self.sync_history if s['success']]
        failed_syncs = [s for s in self.sync_history if not s['success']]
        
        stats = {
            'total_syncs': len(self.sync_history),
            'successful_syncs': len(successful_syncs),
            'failed_syncs': len(failed_syncs),
            'success_rate': len(successful_syncs) / len(self.sync_history) * 100,
            'average_duration': sum(s['duration_seconds'] for s in successful_syncs) / len(successful_syncs) if successful_syncs else 0,
            'total_events_synced': sum(s.get('events_synced', 0) for s in successful_syncs),
            'total_markets_synced': sum(s.get('markets_synced', 0) for s in successful_syncs),
            'last_24h_syncs': len([s for s in self.sync_history if 
                                 datetime.fromisoformat(s['timestamp']) > datetime.now() - timedelta(days=1)])
        }
        
        return stats
    
    def manual_sync(self) -> Dict:
        """æ‰‹åŠ¨è§¦å‘åŒæ­¥"""
        logger.info("æ‰‹åŠ¨è§¦å‘åŒæ­¥...")
        
        try:
            sync_result = self.synchronizer.sync_all_markets()
            
            manual_record = {
                'timestamp': datetime.now().isoformat(),
                'success': True,
                'manual': True,
                'events_synced': self.synchronizer.sync_stats.get('total_events', 0),
                'markets_synced': self.synchronizer.sync_stats.get('total_markets', 0),
                'tags_processed': self.synchronizer.sync_stats.get('total_tags', 0)
            }
            
            self.sync_history.append(manual_record)
            self.last_sync_result = manual_record
            
            logger.info("æ‰‹åŠ¨åŒæ­¥å®Œæˆ")
            return manual_record
            
        except Exception as e:
            manual_record = {
                'timestamp': datetime.now().isoformat(),
                'success': False,
                'manual': True,
                'error': str(e)
            }
            
            self.sync_history.append(manual_record)
            self.last_sync_result = manual_record
            
            logger.error(f"æ‰‹åŠ¨åŒæ­¥å¤±è´¥: {e}")
            return manual_record
    
    def save_sync_history(self, filepath: str = None):
        """ä¿å­˜åŒæ­¥å†å²"""
        if not filepath:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filepath = os.path.join(self.config['sync_settings']['data_dir'], 
                                  "reports", f"sync_history_{timestamp}.json")
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        history_data = {
            'export_timestamp': datetime.now().isoformat(),
            'total_records': len(self.sync_history),
            'statistics': self.get_sync_statistics(),
            'history': self.sync_history
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(history_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"åŒæ­¥å†å²å·²ä¿å­˜åˆ°: {filepath}")
        return filepath

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='PolymarketåŒæ­¥è°ƒåº¦å™¨')
    parser.add_argument('--action', choices=['start', 'stop', 'status', 'sync', 'stats', 'history'], 
                       default='status', help='æ‰§è¡Œçš„æ“ä½œ')
    parser.add_argument('--config', default='sync_config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--daemon', action='store_true', help='ä»¥å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼è¿è¡Œ')
    
    args = parser.parse_args()
    
    scheduler = SyncScheduler(config_file=args.config)
    
    if args.action == 'start':
        scheduler.start_scheduler()
        
        if args.daemon:
            logger.info("ä»¥å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼è¿è¡Œï¼ŒæŒ‰ Ctrl+C åœæ­¢")
            try:
                while True:
                    time.sleep(1)
            except KeyboardInterrupt:
                scheduler.stop_scheduler()
        else:
            logger.info("è°ƒåº¦å™¨å·²å¯åŠ¨ï¼Œä½¿ç”¨ 'stop' å‘½ä»¤åœæ­¢")
    
    elif args.action == 'stop':
        scheduler.stop_scheduler()
    
    elif args.action == 'status':
        status = scheduler.get_schedule_status()
        print(f"è°ƒåº¦å™¨çŠ¶æ€: {'è¿è¡Œä¸­' if status['is_running'] else 'å·²åœæ­¢'}")
        print(f"è°ƒåº¦ä»»åŠ¡æ•°: {status['jobs_count']}")
        
        if status['jobs']:
            print("\nè°ƒåº¦ä»»åŠ¡:")
            for i, job in enumerate(status['jobs'], 1):
                print(f"  {i}. {job['job']}")
                print(f"     ä¸‹æ¬¡è¿è¡Œ: {job['next_run'] or 'N/A'}")
        
        if status['last_sync_result']:
            result = status['last_sync_result']
            print(f"\næœ€ååŒæ­¥:")
            print(f"  æ—¶é—´: {result['timestamp']}")
            print(f"  çŠ¶æ€: {'æˆåŠŸ' if result['success'] else 'å¤±è´¥'}")
            if result['success']:
                print(f"  äº‹ä»¶: {result.get('events_synced', 0)}")
                print(f"  å¸‚åœº: {result.get('markets_synced', 0)}")
    
    elif args.action == 'sync':
        result = scheduler.manual_sync()
        if result['success']:
            print("æ‰‹åŠ¨åŒæ­¥æˆåŠŸ")
            print(f"  äº‹ä»¶: {result.get('events_synced', 0)}")
            print(f"  å¸‚åœº: {result.get('markets_synced', 0)}")
        else:
            print(f"æ‰‹åŠ¨åŒæ­¥å¤±è´¥: {result.get('error', 'Unknown error')}")
    
    elif args.action == 'stats':
        stats = scheduler.get_sync_statistics()
        print("åŒæ­¥ç»Ÿè®¡:")
        for key, value in stats.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.2f}")
            else:
                print(f"  {key}: {value}")
    
    elif args.action == 'history':
        filepath = scheduler.save_sync_history()
        print(f"åŒæ­¥å†å²å·²å¯¼å‡ºåˆ°: {filepath}")

if __name__ == "__main__":
    main()