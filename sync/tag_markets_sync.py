#!/usr/bin/env python3
"""
æ ‡ç­¾å¸‚åœºåŒæ­¥å™¨ - é€šç”¨çš„æ ‡ç­¾å¸‚åœºæœç´¢å’ŒåŒæ­¥å·¥å…·
æ”¯æŒé€šè¿‡æ ‡ç­¾IDæœç´¢ä»»ä½•æ ‡ç­¾ä¸‹çš„æœªå…³é—­å¸‚åœºæ•°æ®
æ”¯æŒé€šè¿‡å…³é”®è¯æœç´¢ eventï¼Œç„¶åæ‰¾åˆ°å¯¹åº”çš„ market
"""

import json
import os
import csv
import time
import argparse
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import requests
from dateutil import parser as date_parser

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TagMarketsSync:
    def __init__(self, data_dir: str = "./data", max_retries: int = 5):
        self.data_dir = data_dir
        self.max_retries = max_retries
        
        # æ ‡å‡†è¯·æ±‚å¤´
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "en-US,en;q=0.9",
            "Referer": "https://polymarket.com/",
            "Connection": "keep-alive",
            "Cache-Control": "no-cache"
        }
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        for subdir in ["tags", "sync_logs", "reports"]:
            os.makedirs(os.path.join(data_dir, subdir), exist_ok=True)
    
    def check_network_connectivity(self) -> bool:
        """
        æ£€æŸ¥ç½‘ç»œè¿æ¥æ€§
        """
        try:
            # å°è¯•è¿æ¥åˆ°ä¸€ä¸ªç®€å•çš„ç«¯ç‚¹
            response = requests.get("https://httpbin.org/status/200", timeout=10)
            return response.status_code == 200
        except:
            try:
                # å¤‡ç”¨æ£€æŸ¥
                response = requests.get("https://www.google.com", timeout=10)
                return response.status_code == 200
            except:
                return False
    
    def handle_connection_error(self, error: Exception, attempt: int) -> bool:
        """
        å¤„ç†è¿æ¥é”™è¯¯å¹¶å†³å®šæ˜¯å¦é‡è¯•
        
        Returns:
            bool: æ˜¯å¦åº”è¯¥é‡è¯•
        """
        error_str = str(error).lower()
        
        if "connection reset by peer" in error_str or "connection aborted" in error_str:
            logger.warning(f"ğŸ”Œ è¿æ¥è¢«é‡ç½® (ç¬¬{attempt + 1}æ¬¡å°è¯•)")
            logger.info("ğŸ’¡ è¿™é€šå¸¸æ˜¯ç”±äº:")
            logger.info("   - ç½‘ç»œä¸ç¨³å®š")
            logger.info("   - æœåŠ¡å™¨ä¸´æ—¶é™åˆ¶")
            logger.info("   - é˜²ç«å¢™æˆ–ä»£ç†è®¾ç½®")
            
            if not self.check_network_connectivity():
                logger.error("âŒ ç½‘ç»œè¿æ¥æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®")
                return False
            else:
                logger.info("âœ… åŸºç¡€ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œç»§ç»­é‡è¯•...")
                return attempt < self.max_retries - 1
        
        elif "timeout" in error_str:
            logger.warning(f"â±ï¸  è¯·æ±‚è¶…æ—¶ (ç¬¬{attempt + 1}æ¬¡å°è¯•)")
            return attempt < self.max_retries - 1
        
        elif "ssl" in error_str or "certificate" in error_str:
            logger.error("ğŸ”’ SSLè¯ä¹¦é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿæ—¶é—´å’Œè¯ä¹¦è®¾ç½®")
            return False
        
    def print_connection_troubleshooting(self):
        """
        æ‰“å°è¿æ¥é—®é¢˜çš„æ•…éšœæ’é™¤å»ºè®®
        """
        print("\n" + "="*60)
        print("ğŸ”§ è¿æ¥é—®é¢˜æ•…éšœæ’é™¤å»ºè®®")
        print("="*60)
        print("1. ğŸŒ æ£€æŸ¥ç½‘ç»œè¿æ¥:")
        print("   - ç¡®ä¿äº’è”ç½‘è¿æ¥æ­£å¸¸")
        print("   - å°è¯•è®¿é—®å…¶ä»–ç½‘ç«™ç¡®è®¤ç½‘ç»œçŠ¶æ€")
        print()
        print("2. ğŸ”¥ æ£€æŸ¥é˜²ç«å¢™è®¾ç½®:")
        print("   - ç¡®ä¿Pythonç¨‹åºå¯ä»¥è®¿é—®å¤–ç½‘")
        print("   - æ£€æŸ¥å…¬å¸/å­¦æ ¡ç½‘ç»œæ˜¯å¦æœ‰é™åˆ¶")
        print()
        print("3. ğŸ• è°ƒæ•´è¯·æ±‚å‚æ•°:")
        print("   - å‡å°‘å¹¶å‘è¯·æ±‚æ•°é‡")
        print("   - å¢åŠ è¯·æ±‚é—´éš”æ—¶é—´")
        print("   - ä½¿ç”¨ --debug å‚æ•°æŸ¥çœ‹è¯¦ç»†æ—¥å¿—")
        print()
        print("4. ğŸ”„ é‡è¯•ç­–ç•¥:")
        print("   - ç¨‹åºä¼šè‡ªåŠ¨é‡è¯•å¤±è´¥çš„è¯·æ±‚")
        print("   - å¯ä»¥ç¨åå†æ¬¡è¿è¡Œç¨‹åº")
        print("   - ä½¿ç”¨ --no-save å‚æ•°ä»…æµ‹è¯•è¿æ¥")
        print()
        print("5. ğŸ“ å¦‚æœé—®é¢˜æŒç»­:")
        print("   - æ£€æŸ¥ Polymarket API æœåŠ¡çŠ¶æ€")
        print("   - å°è¯•ä½¿ç”¨VPNæˆ–æ›´æ¢ç½‘ç»œç¯å¢ƒ")
        print("="*60)

    def run_connection_test(self) -> bool:
        """
        è¿è¡Œè¿æ¥æµ‹è¯•
        """
        print("ğŸ” å¼€å§‹è¿æ¥æµ‹è¯•...")
        
        # æµ‹è¯•åŸºç¡€ç½‘ç»œè¿æ¥
        print("1. æµ‹è¯•åŸºç¡€ç½‘ç»œè¿æ¥...")
        if self.check_network_connectivity():
            print("   âœ… åŸºç¡€ç½‘ç»œè¿æ¥æ­£å¸¸")
        else:
            print("   âŒ åŸºç¡€ç½‘ç»œè¿æ¥å¤±è´¥")
            return False
        
        # æµ‹è¯• Polymarket API è¿æ¥
        print("2. æµ‹è¯• Polymarket API è¿æ¥...")
        test_url = "https://gamma-api.polymarket.com/tags"
        result = self.make_api_request(test_url)
        
        if result:
            print("   âœ… Polymarket API è¿æ¥æˆåŠŸ")
            print(f"   ğŸ“Š è·å–åˆ° {len(result) if isinstance(result, list) else 'N/A'} æ¡æ•°æ®")
            return True
        else:
            print("   âŒ Polymarket API è¿æ¥å¤±è´¥")
            self.print_connection_troubleshooting()
            return False

    def get_available_tags(self) -> List[Dict]:
        """
        è·å–å¯ç”¨çš„æ ‡ç­¾åˆ—è¡¨
        
        Returns:
            List[Dict]: æ ‡ç­¾åˆ—è¡¨ï¼ŒåŒ…å«idå’Œname
        """
        url = "https://gamma-api.polymarket.com/tags"
        
        logger.info("ğŸ·ï¸  è·å–å¯ç”¨æ ‡ç­¾åˆ—è¡¨...")
        
        data = self.make_api_request(url)
        
        if not data:
            logger.warning("æ— æ³•è·å–æ ‡ç­¾åˆ—è¡¨")
            return []
        
        tags = data if isinstance(data, list) else data.get('data', [])
        
        logger.info(f"âœ… è·å–åˆ° {len(tags)} ä¸ªå¯ç”¨æ ‡ç­¾")
        
        # æ‰“å°å‰20ä¸ªæ ‡ç­¾ä½œä¸ºç¤ºä¾‹
        if tags:
            print("\nğŸ“‹ å¯ç”¨æ ‡ç­¾ç¤ºä¾‹ (å‰20ä¸ª):")
            print("   æ ‡ç­¾ç»“æ„ç¤ºä¾‹:", json.dumps(tags[0], indent=2) if tags else "æ— æ•°æ®")
            
            for i, tag in enumerate(tags[:20]):
                # å°è¯•ä¸åŒçš„å­—æ®µå
                tag_id = tag.get('id', tag.get('tagId', 'N/A'))
                tag_name = tag.get('name', tag.get('label', tag.get('title', tag.get('slug', 'N/A'))))
                tag_count = tag.get('count', tag.get('marketCount', ''))
                
                count_str = f" ({tag_count} å¸‚åœº)" if tag_count else ""
                print(f"   {i+1:2d}. ID: {tag_id:<20} åç§°: {tag_name}{count_str}")
            
            if len(tags) > 20:
                print(f"   ... è¿˜æœ‰ {len(tags) - 20} ä¸ªæ ‡ç­¾")
        
        return tags

    def save_tags_to_file(self, tags: List[Dict]) -> str:
        """
        å°†æ ‡ç­¾åˆ—è¡¨ä¿å­˜åˆ°æ–‡ä»¶
        
        Args:
            tags: æ ‡ç­¾åˆ—è¡¨
            
        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜ä¸ºCSVæ ¼å¼
        csv_filename = f"available_tags_{timestamp}.csv"
        csv_path = os.path.join(self.data_dir, "tags", csv_filename)
        
        # CSVæ ‡é¢˜
        csv_headers = ['id', 'label', 'slug', 'createdAt', 'updatedAt', 'requiresTranslation']
        
        with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(csv_headers)
            
            for tag in tags:
                row = [
                    tag.get('id', ''),
                    tag.get('label', ''),
                    tag.get('slug', ''),
                    tag.get('createdAt', ''),
                    tag.get('updatedAt', ''),
                    tag.get('requiresTranslation', '')
                ]
                writer.writerow(row)
        
        logger.info(f"ğŸ’¾ æ ‡ç­¾CSVæ•°æ®å·²ä¿å­˜åˆ°: {csv_path}")
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼
        json_filename = f"available_tags_{timestamp}.json"
        json_path = os.path.join(self.data_dir, "tags", json_filename)
        
        with open(json_path, 'w', encoding='utf-8') as jsonfile:
            json.dump(tags, jsonfile, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ æ ‡ç­¾JSONæ•°æ®å·²ä¿å­˜åˆ°: {json_path}")
        
        # åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æ ‡ç­¾æ˜ å°„æ–‡ä»¶ï¼Œæ–¹ä¾¿æŸ¥æ‰¾
        mapping_filename = f"tag_id_mapping_{timestamp}.json"
        mapping_path = os.path.join(self.data_dir, "tags", mapping_filename)
        
        tag_mapping = {}
        for tag in tags:
            tag_id = tag.get('id', '')
            tag_label = tag.get('label', '')
            tag_slug = tag.get('slug', '')
            if tag_id:
                tag_mapping[tag_id] = {
                    'label': tag_label,
                    'slug': tag_slug
                }
        
        with open(mapping_path, 'w', encoding='utf-8') as mappingfile:
            json.dump(tag_mapping, mappingfile, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ æ ‡ç­¾æ˜ å°„æ–‡ä»¶å·²ä¿å­˜åˆ°: {mapping_path}")
        
        return csv_path

    def sync_all_tags(self) -> Dict[str, Any]:
        """
        åŒæ­¥æ‰€æœ‰å¯ç”¨æ ‡ç­¾åˆ°æ–‡ä»¶
        
        Returns:
            Dict: åŒæ­¥ç»“æœ
        """
        start_time = datetime.now()
        logger.info("ğŸš€ å¼€å§‹åŒæ­¥æ‰€æœ‰å¯ç”¨æ ‡ç­¾")
        
        try:
            # è·å–æ‰€æœ‰æ ‡ç­¾
            tags = self.get_available_tags()
            
            if not tags:
                return {
                    "success": False,
                    "error": "æ— æ³•è·å–æ ‡ç­¾åˆ—è¡¨",
                    "tags_count": 0
                }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            csv_file = self.save_tags_to_file(tags)
            
            # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            print("\n" + "="*80)
            print("ğŸ·ï¸  æ ‡ç­¾åŒæ­¥æŠ¥å‘Š")
            print("="*80)
            print(f"ğŸ• åŒæ­¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ğŸ¯ åŒæ­¥æ ‡ç­¾: {len(tags)} ä¸ª")
            
            # æŒ‰æ ‡ç­¾åç§°é•¿åº¦åˆ†ç»„ç»Ÿè®¡
            length_stats = {}
            for tag in tags:
                label = tag.get('label', '')
                length = len(label) if label else 0
                if length == 0:
                    key = "æ— æ ‡ç­¾å"
                elif length <= 5:
                    key = "çŸ­æ ‡ç­¾(â‰¤5å­—ç¬¦)"
                elif length <= 15:
                    key = "ä¸­ç­‰æ ‡ç­¾(6-15å­—ç¬¦)"
                else:
                    key = "é•¿æ ‡ç­¾(>15å­—ç¬¦)"
                
                length_stats[key] = length_stats.get(key, 0) + 1
            
            if length_stats:
                print(f"\nğŸ“Š æ ‡ç­¾é•¿åº¦åˆ†å¸ƒ:")
                for category, count in sorted(length_stats.items(), key=lambda x: x[1], reverse=True):
                    print(f"   {category}: {count} ä¸ª")
            
            # æ˜¾ç¤ºæœ€æ–°çš„æ ‡ç­¾
            recent_tags = sorted(tags, key=lambda x: x.get('createdAt', ''), reverse=True)[:10]
            if recent_tags:
                print(f"\nğŸ†• æœ€æ–°åˆ›å»ºçš„æ ‡ç­¾ (å‰10ä¸ª):")
                for i, tag in enumerate(recent_tags):
                    created_at = tag.get('createdAt', '')
                    if created_at:
                        try:
                            created_time = date_parser.parse(created_at)
                            time_str = created_time.strftime('%Y-%m-%d')
                        except:
                            time_str = created_at[:10]
                    else:
                        time_str = "æœªçŸ¥"
                    
                    print(f"   {i+1:2d}. [{time_str}] ID: {tag.get('id', 'N/A'):<10} åç§°: {tag.get('label', 'N/A')}")
            
            print("="*80)
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            return {
                "success": True,
                "tags_count": len(tags),
                "csv_file": csv_file,
                "duration_seconds": duration
            }
            
        except Exception as e:
            logger.error(f"æ ‡ç­¾åŒæ­¥å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "tags_count": 0
            }

    def make_api_request(self, url: str, params: Optional[Dict] = None, timeout: int = 30) -> Optional[Dict]:
        """
        å‘é€APIè¯·æ±‚ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†
        """
        for attempt in range(self.max_retries):
            try:
                response = requests.get(url, params=params, headers=self.headers, timeout=timeout)
                
                # å¤„ç†ä¸åŒçš„HTTPçŠ¶æ€ç 
                if response.status_code == 500:
                    logger.warning(f"æœåŠ¡å™¨é”™è¯¯ (500) - ç¬¬{attempt + 1}æ¬¡é‡è¯•ï¼Œç­‰å¾…5ç§’...")
                    time.sleep(5)
                    continue
                elif response.status_code == 429:
                    logger.warning(f"è¯·æ±‚é™åˆ¶ (429) - ç¬¬{attempt + 1}æ¬¡é‡è¯•ï¼Œç­‰å¾…10ç§’...")
                    time.sleep(10)
                    continue
                elif response.status_code == 404:
                    logger.warning(f"èµ„æºæœªæ‰¾åˆ° (404): {url}")
                    return None
                elif response.status_code != 200:
                    logger.error(f"APIé”™è¯¯ {response.status_code}: {response.text[:200]}")
                    if attempt < self.max_retries - 1:
                        logger.info(f"ç¬¬{attempt + 1}æ¬¡é‡è¯•ï¼Œç­‰å¾…3ç§’...")
                        time.sleep(3)
                        continue
                    else:
                        return None
                
                # å°è¯•è§£æJSON
                try:
                    return response.json()
                except json.JSONDecodeError as e:
                    logger.error(f"JSONè§£æå¤±è´¥: {e}")
                    if response.text.strip():
                        logger.error(f"å“åº”å†…å®¹: {response.text[:500]}")
                    return None
                    
            except requests.exceptions.ConnectionError as e:
                if not self.handle_connection_error(e, attempt):
                    return None
                wait_time = min(10 * (attempt + 1), 30)  # é€’å¢ç­‰å¾…æ—¶é—´ï¼Œæœ€å¤š30ç§’
                logger.info(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
                continue
            except requests.exceptions.Timeout as e:
                logger.error(f"è¯·æ±‚è¶…æ—¶ (ç¬¬{attempt + 1}æ¬¡): {e}")
                if attempt < self.max_retries - 1:
                    logger.info(f"å¢åŠ è¶…æ—¶æ—¶é—´å¹¶é‡è¯•...")
                    timeout = min(timeout * 1.5, 60)  # å¢åŠ è¶…æ—¶æ—¶é—´
                    time.sleep(5)
                    continue
                else:
                    return None
            except requests.exceptions.RequestException as e:
                logger.error(f"ç½‘ç»œé”™è¯¯ (ç¬¬{attempt + 1}æ¬¡): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(5)
                    continue
                else:
                    return None
            except Exception as e:
                logger.error(f"æ„å¤–é”™è¯¯ (ç¬¬{attempt + 1}æ¬¡): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(3)
                    continue
                else:
                    return None
        
        return None

    def search_markets_by_tag(self, tag_id: str, batch_size: int = 100, max_markets: int = 1000) -> List[Dict]:
        """
        é€šè¿‡æ ‡ç­¾IDæœç´¢å¸‚åœº
        
        Args:
            tag_id: æ ‡ç­¾ID
            batch_size: æ‰¹æ¬¡å¤§å°
            max_markets: æœ€å¤§æœç´¢å¸‚åœºæ•°é‡
            
        Returns:
            List[Dict]: åŒ¹é…çš„å¸‚åœºåˆ—è¡¨
        """
        base_url = "https://gamma-api.polymarket.com/markets"
        markets = []
        offset = 0
        
        logger.info(f"ğŸ·ï¸  æœç´¢æ ‡ç­¾ID '{tag_id}' çš„å¸‚åœº...")
        
        while len(markets) < max_markets:
            params = {
                'tagId': tag_id,     # ä½¿ç”¨tagIdè€Œä¸æ˜¯tag
                'closed': 'false',   # åªè·å–æœªå…³é—­çš„å¸‚åœº
                'active': 'true',    # åªè·å–æ´»è·ƒå¸‚åœº
                'limit': batch_size,
                'offset': offset,
                'order': 'volumeNum',  # æŒ‰äº¤æ˜“é‡æ’åº
                'ascending': 'false'   # é™åºæ’åˆ—
            }
            
            data = self.make_api_request(base_url, params)
            
            if not data:
                logger.warning(f"æ— æ³•è·å–æ ‡ç­¾ID '{tag_id}' åç§»é‡ {offset} çš„æ•°æ®")
                break
            
            batch_markets = data if isinstance(data, list) else data.get('data', [])
            
            if not batch_markets:
                logger.info(f"æ ‡ç­¾ID '{tag_id}' åœ¨åç§»é‡ {offset} æ²¡æœ‰æ‰¾åˆ°æ›´å¤šå¸‚åœº")
                break
            
            markets.extend(batch_markets)
            logger.info(f"æ ‡ç­¾ID '{tag_id}' è·å–äº† {len(batch_markets)} ä¸ªå¸‚åœºï¼Œæ€»è®¡ {len(markets)} ä¸ª")
            
            offset += len(batch_markets)
            
            # å¦‚æœè¿™æ‰¹æ•°æ®å°‘äºæ‰¹æ¬¡å¤§å°ï¼Œè¯´æ˜åˆ°è¾¾æœ«å°¾
            if len(batch_markets) < batch_size:
                break
        
        logger.info(f"âœ… æ ‡ç­¾ID '{tag_id}' æœç´¢å®Œæˆ: æ‰¾åˆ° {len(markets)} ä¸ªå¸‚åœº")
        return markets

    def search_markets_by_multiple_tags(self, tag_ids: List[str], batch_size: int = 100) -> List[Dict]:
        """
        æœç´¢å¤šä¸ªæ ‡ç­¾IDçš„å¸‚åœºå¹¶å»é‡
        
        Args:
            tag_ids: æ ‡ç­¾IDåˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            List[Dict]: å»é‡åçš„å¸‚åœºåˆ—è¡¨
        """
        all_markets = []
        seen_ids = set()
        
        for tag_id in tag_ids:
            logger.info(f"ğŸ” æœç´¢æ ‡ç­¾ID: {tag_id}")
            tag_markets = self.search_markets_by_tag(tag_id, batch_size)
            
            # å»é‡æ·»åŠ 
            for market in tag_markets:
                market_id = market.get('id')
                if market_id and market_id not in seen_ids:
                    all_markets.append(market)
                    seen_ids.add(market_id)
                    logger.debug(f"æ·»åŠ å¸‚åœº: {market.get('question', 'Unknown')[:60]}...")
        
        logger.info(f"ğŸ¯ å¤šæ ‡ç­¾IDæœç´¢å®Œæˆ: æ€»å…±æ‰¾åˆ° {len(all_markets)} ä¸ªå”¯ä¸€å¸‚åœº")
        return all_markets

    def search_events_by_keyword(self, keyword: str, batch_size: int = 100, max_events: int = 500) -> List[Dict]:
        """
        é€šè¿‡å…³é”®è¯æœç´¢ events
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            batch_size: æ‰¹æ¬¡å¤§å°
            max_events: æœ€å¤§æœç´¢äº‹ä»¶æ•°é‡
            
        Returns:
            List[Dict]: åŒ¹é…çš„äº‹ä»¶åˆ—è¡¨
        """
        base_url = "https://gamma-api.polymarket.com/events"
        matching_events = []
        offset = 0
        total_checked = 0
        
        logger.info(f"ğŸ” æœç´¢å…³é”®è¯ '{keyword}' çš„äº‹ä»¶...")
        
        while total_checked < max_events:
            params = {
                'closed': 'false',
                'active': 'true',
                'limit': batch_size,
                'offset': offset,
                'order': 'startDate',
                'ascending': 'false'
            }
            
            data = self.make_api_request(base_url, params)
            
            if not data:
                break
            
            events = data if isinstance(data, list) else data.get('data', [])
            
            if not events:
                break
            
            # æ£€æŸ¥æ¯ä¸ªäº‹ä»¶çš„æ ‡é¢˜å’Œæè¿°æ˜¯å¦åŒ…å«å…³é”®è¯
            for event in events:
                total_checked += 1
                title = str(event.get('title', '')).lower()
                description = str(event.get('description', '')).lower()
                slug = str(event.get('slug', '')).lower()
                
                if (keyword.lower() in title or 
                    keyword.lower() in description or 
                    keyword.lower() in slug):
                    matching_events.append(event)
                    logger.debug(f"æ‰¾åˆ°åŒ¹é…äº‹ä»¶: {event.get('title', 'Unknown')[:60]}...")
            
            offset += len(events)
            
            if len(events) < batch_size:
                break
        
        logger.info(f"âœ… å…³é”®è¯ '{keyword}' äº‹ä»¶æœç´¢å®Œæˆ: æ£€æŸ¥äº† {total_checked} ä¸ªäº‹ä»¶ï¼Œæ‰¾åˆ° {len(matching_events)} ä¸ªåŒ¹é…")
        return matching_events
    
    def get_markets_by_event_id(self, event_id: str) -> List[Dict]:
        """
        é€šè¿‡äº‹ä»¶IDè·å–ç›¸å…³çš„å¸‚åœº
        
        Args:
            event_id: äº‹ä»¶ID
            
        Returns:
            List[Dict]: è¯¥äº‹ä»¶ä¸‹çš„å¸‚åœºåˆ—è¡¨
        """
        base_url = "https://gamma-api.polymarket.com/markets"
        
        params = {
            'eventId': event_id,
            'closed': 'false',
            'active': 'true',
            'limit': 100,
            'order': 'volumeNum',
            'ascending': 'false'
        }
        
        logger.info(f"ğŸ¯ è·å–äº‹ä»¶ID '{event_id}' çš„å¸‚åœº...")
        
        data = self.make_api_request(base_url, params)
        
        if not data:
            logger.warning(f"æ— æ³•è·å–äº‹ä»¶ID '{event_id}' çš„å¸‚åœºæ•°æ®")
            return []
        
        markets = data if isinstance(data, list) else data.get('data', [])
        
        logger.info(f"âœ… äº‹ä»¶ID '{event_id}' æ‰¾åˆ° {len(markets)} ä¸ªå¸‚åœº")
        return markets
    
    def search_markets_by_event_keyword(self, keyword: str, max_events: int = 100) -> List[Dict]:
        """
        é€šè¿‡å…³é”®è¯æœç´¢äº‹ä»¶ï¼Œç„¶åè·å–è¿™äº›äº‹ä»¶ä¸‹çš„æ‰€æœ‰å¸‚åœº
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            max_events: æœ€å¤§æœç´¢äº‹ä»¶æ•°é‡
            
        Returns:
            List[Dict]: æ‰€æœ‰åŒ¹é…äº‹ä»¶ä¸‹çš„å¸‚åœºåˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¼€å§‹é€šè¿‡å…³é”®è¯ '{keyword}' æœç´¢äº‹ä»¶å’Œå¸‚åœº...")
        
        # 1. å…ˆæœç´¢åŒ¹é…çš„äº‹ä»¶
        events = self.search_events_by_keyword(keyword, max_events=max_events)
        
        if not events:
            logger.warning(f"å…³é”®è¯ '{keyword}' æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„äº‹ä»¶")
            return []
        
        # 2. ä¸ºæ¯ä¸ªäº‹ä»¶è·å–å¸‚åœº
        all_markets = []
        seen_market_ids = set()
        
        for i, event in enumerate(events):
            event_id = event.get('id')
            event_title = event.get('title', 'Unknown')
            
            if not event_id:
                logger.warning(f"äº‹ä»¶ '{event_title}' æ²¡æœ‰IDï¼Œè·³è¿‡")
                continue
            
            logger.info(f"ğŸ“Š å¤„ç†äº‹ä»¶ {i+1}/{len(events)}: {event_title[:50]}...")
            
            # è·å–è¯¥äº‹ä»¶çš„å¸‚åœº
            event_markets = self.get_markets_by_event_id(event_id)
            
            # å»é‡æ·»åŠ å¸‚åœº
            for market in event_markets:
                market_id = market.get('id')
                if market_id and market_id not in seen_market_ids:
                    # æ·»åŠ äº‹ä»¶ä¿¡æ¯åˆ°å¸‚åœºæ•°æ®ä¸­
                    market['event_info'] = {
                        'event_id': event_id,
                        'event_title': event_title,
                        'event_slug': event.get('slug', ''),
                        'event_description': event.get('description', '')
                    }
                    all_markets.append(market)
                    seen_market_ids.add(market_id)
        
        logger.info(f"ğŸ¯ å…³é”®è¯ '{keyword}' äº‹ä»¶æœç´¢å®Œæˆ: ä» {len(events)} ä¸ªäº‹ä»¶ä¸­æ‰¾åˆ° {len(all_markets)} ä¸ªå”¯ä¸€å¸‚åœº")
        return all_markets
    
    def search_markets_by_keyword_direct(self, keyword: str, batch_size: int = 100, max_markets: int = 1000) -> List[Dict]:
        """
        é€šè¿‡å…³é”®è¯ç›´æ¥æœç´¢å¸‚åœºï¼ˆåœ¨é—®é¢˜å†…å®¹ä¸­æœç´¢ï¼‰
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            batch_size: æ‰¹æ¬¡å¤§å°
            max_markets: æœ€å¤§æœç´¢å¸‚åœºæ•°é‡
            
        Returns:
            List[Dict]: åŒ¹é…çš„å¸‚åœºåˆ—è¡¨
        """
        base_url = "https://gamma-api.polymarket.com/markets"
        matching_markets = []
        offset = 0
        total_checked = 0
        
        logger.info(f"ğŸ” ç›´æ¥æœç´¢å…³é”®è¯ '{keyword}' çš„å¸‚åœº...")
        
        while total_checked < max_markets:
            params = {
                'closed': 'false',
                'active': 'true',
                'limit': batch_size,
                'offset': offset,
                'order': 'volumeNum',
                'ascending': 'false'
            }
            
            data = self.make_api_request(base_url, params)
            
            if not data:
                break
            
            markets = data if isinstance(data, list) else data.get('data', [])
            
            if not markets:
                break
            
            # æ£€æŸ¥æ¯ä¸ªå¸‚åœºçš„é—®é¢˜æ˜¯å¦åŒ…å«å…³é”®è¯
            for market in markets:
                total_checked += 1
                question = str(market.get('question', '')).lower()
                
                if keyword.lower() in question:
                    matching_markets.append(market)
                    logger.debug(f"æ‰¾åˆ°åŒ¹é…å¸‚åœº: {market.get('question', 'Unknown')[:60]}...")
            
            offset += len(markets)
            
            if len(markets) < batch_size:
                break
        
        logger.info(f"âœ… å…³é”®è¯ '{keyword}' ç›´æ¥å¸‚åœºæœç´¢å®Œæˆ: æ£€æŸ¥äº† {total_checked} ä¸ªå¸‚åœºï¼Œæ‰¾åˆ° {len(matching_markets)} ä¸ªåŒ¹é…")
        return matching_markets
    
    def search_markets_by_keyword(self, keyword: str, search_method: str = 'both', **kwargs) -> List[Dict]:
        """
        é€šè¿‡å…³é”®è¯æœç´¢å¸‚åœºçš„ç»Ÿä¸€æ¥å£
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            search_method: æœç´¢æ–¹æ³• ('event', 'direct', 'both')
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            List[Dict]: åŒ¹é…çš„å¸‚åœºåˆ—è¡¨
        """
        all_markets = []
        seen_market_ids = set()
        
        if search_method in ['event', 'both']:
            # é€šè¿‡äº‹ä»¶æœç´¢
            logger.info(f"ğŸ¯ é€šè¿‡äº‹ä»¶æœç´¢å…³é”®è¯ '{keyword}'...")
            event_markets = self.search_markets_by_event_keyword(keyword, **kwargs)
            
            for market in event_markets:
                market_id = market.get('id')
                if market_id and market_id not in seen_market_ids:
                    all_markets.append(market)
                    seen_market_ids.add(market_id)
        
        if search_method in ['direct', 'both']:
            # ç›´æ¥æœç´¢å¸‚åœº
            logger.info(f"ğŸ” ç›´æ¥æœç´¢å…³é”®è¯ '{keyword}'...")
            direct_markets = self.search_markets_by_keyword_direct(keyword, **kwargs)
            
            for market in direct_markets:
                market_id = market.get('id')
                if market_id and market_id not in seen_market_ids:
                    all_markets.append(market)
                    seen_market_ids.add(market_id)
        
        logger.info(f"ğŸ¯ å…³é”®è¯ '{keyword}' ç»¼åˆæœç´¢å®Œæˆ: æ‰¾åˆ° {len(all_markets)} ä¸ªå”¯ä¸€å¸‚åœº")
        return all_markets

    def save_markets_data(self, markets: List[Dict], tag_name: str) -> str:
        """
        ä¿å­˜å¸‚åœºæ•°æ®åˆ°CSVæ–‡ä»¶
        
        Args:
            markets: å¸‚åœºåˆ—è¡¨
            tag_name: æ ‡ç­¾åç§°æˆ–IDï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼‰
            
        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_tag_name = "".join(c for c in tag_name if c.isalnum() or c in ('-', '_')).lower()
        filename = f"{safe_tag_name}_markets_{timestamp}.csv"
        full_path = os.path.join(self.data_dir, "tags", filename)
        
        # CSVæ ‡é¢˜ - æ·»åŠ äº‹ä»¶ç›¸å…³å­—æ®µ
        headers = [
            'id', 'question', 'slug', 'category', 'tags', 'clobTokenIds', 'outcomes', 
            'outcomePrices', 'conditionId', 'active', 'closed', 'volumeNum', 
            'volume24hr', 'liquidity', 'liquidityNum', 'endDate', 
            'orderPriceMinTickSize', 'orderMinSize', 'resolutionSource', 
            'acceptingOrders', 'openInterest', 'createdAt', 'updatedAt',
            # äº‹ä»¶ç›¸å…³å­—æ®µ
            'event_id', 'event_title', 'event_slug', 'event_description'
        ]
        
        with open(full_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(headers)
            
            for market in markets:
                try:
                    # å¤„ç†JSONå­—æ®µ
                    def safe_json_field(field_value):
                        """å®‰å…¨å¤„ç†JSONå­—æ®µï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²"""
                        if isinstance(field_value, (list, dict)):
                            return json.dumps(field_value)
                        elif isinstance(field_value, str):
                            return field_value
                        else:
                            return str(field_value) if field_value is not None else ''
                    
                    # è·å–äº‹ä»¶ä¿¡æ¯
                    event_info = market.get('event_info', {})
                    
                    # åˆ›å»ºæ•°æ®è¡Œ
                    row = [
                        market.get('id', ''),
                        market.get('question', ''),
                        market.get('slug', ''),
                        market.get('category', ''),
                        safe_json_field(market.get('tags', '')),
                        safe_json_field(market.get('clobTokenIds', '')),
                        safe_json_field(market.get('outcomes', '')),
                        safe_json_field(market.get('outcomePrices', '')),
                        market.get('conditionId', ''),
                        market.get('active', ''),
                        market.get('closed', ''),
                        market.get('volumeNum', ''),
                        market.get('volume24hr', ''),
                        market.get('liquidity', ''),
                        market.get('liquidityNum', ''),
                        market.get('endDate', ''),
                        market.get('orderPriceMinTickSize', ''),
                        market.get('orderMinSize', ''),
                        market.get('resolutionSource', ''),
                        market.get('acceptingOrders', ''),
                        market.get('openInterest', ''),
                        market.get('createdAt', ''),
                        market.get('updatedAt', ''),
                        # äº‹ä»¶ä¿¡æ¯
                        event_info.get('event_id', ''),
                        event_info.get('event_title', ''),
                        event_info.get('event_slug', ''),
                        event_info.get('event_description', '')
                    ]
                    
                    writer.writerow(row)
                    
                except Exception as e:
                    logger.error(f"å¤„ç†å¸‚åœº {market.get('id', 'unknown')} æ—¶å‡ºé”™: {e}")
                    continue
        
        logger.info(f"ğŸ’¾ å¸‚åœºæ•°æ®å·²ä¿å­˜åˆ°: {full_path}")
        return full_path

    def generate_markets_report(self, markets: List[Dict], search_term: str) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¸‚åœºæŠ¥å‘Š
        
        Args:
            markets: å¸‚åœºåˆ—è¡¨
            search_term: æœç´¢è¯ï¼ˆæ ‡ç­¾æˆ–å…³é”®è¯ï¼‰
            
        Returns:
            Dict: æŠ¥å‘Šæ•°æ®
        """
        current_time = datetime.now()
        
        # æŒ‰äº¤æ˜“é‡æ’åº
        sorted_markets = sorted(markets, key=lambda x: float(x.get('volumeNum', 0) or 0), reverse=True)
        
        # åˆ†ç¦»æœ‰äº‹ä»¶ä¿¡æ¯å’Œæ— äº‹ä»¶ä¿¡æ¯çš„å¸‚åœº
        markets_with_events = []
        markets_without_events = []
        events_info = {}
        
        for market in markets:
            event_info = market.get('event_info', {})
            if event_info.get('event_id'):
                markets_with_events.append(market)
                event_id = event_info['event_id']
                if event_id not in events_info:
                    events_info[event_id] = {
                        'event_title': event_info.get('event_title', ''),
                        'event_slug': event_info.get('event_slug', ''),
                        'event_description': event_info.get('event_description', ''),
                        'markets': []
                    }
                events_info[event_id]['markets'].append(market)
            else:
                markets_without_events.append(market)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_volume = 0
        total_liquidity = 0
        categories = {}
        
        for market in markets:
            # ç»Ÿè®¡äº¤æ˜“é‡
            try:
                volume = float(market.get('volumeNum', 0) or 0)
                total_volume += volume
            except:
                pass
                
            # ç»Ÿè®¡æµåŠ¨æ€§
            try:
                liquidity = float(market.get('liquidityNum', 0) or 0)
                total_liquidity += liquidity
            except:
                pass
                
            # ç»Ÿè®¡åˆ†ç±»
            category = market.get('category', 'Unknown')
            if category:
                categories[category] = categories.get(category, 0) + 1
        
        # æ—¶é—´åˆ†æ
        time_ranges = {
            '1å¤©å†…': 0, '1å‘¨å†…': 0, '1æœˆå†…': 0, '3æœˆå†…': 0, '6æœˆå†…': 0, '1å¹´å†…': 0, '1å¹´ä»¥ä¸Š': 0
        }
        
        for market in markets:
            end_date = market.get('endDate', '')
            if end_date:
                try:
                    end_time = date_parser.parse(end_date)
                    current_time_tz = datetime.now(end_time.tzinfo) if end_time.tzinfo else datetime.now()
                    time_diff = end_time - current_time_tz
                    days = time_diff.days
                    
                    if days <= 1:
                        time_ranges['1å¤©å†…'] += 1
                    elif days <= 7:
                        time_ranges['1å‘¨å†…'] += 1
                    elif days <= 30:
                        time_ranges['1æœˆå†…'] += 1
                    elif days <= 90:
                        time_ranges['3æœˆå†…'] += 1
                    elif days <= 180:
                        time_ranges['6æœˆå†…'] += 1
                    elif days <= 365:
                        time_ranges['1å¹´å†…'] += 1
                    else:
                        time_ranges['1å¹´ä»¥ä¸Š'] += 1
                except:
                    pass
        
        # ç”Ÿæˆäº‹ä»¶ç»Ÿè®¡
        events_stats = []
        for event_id, event_data in events_info.items():
            event_markets = event_data['markets']
            event_volume = sum(float(m.get('volumeNum', 0) or 0) for m in event_markets)
            event_liquidity = sum(float(m.get('liquidityNum', 0) or 0) for m in event_markets)
            
            events_stats.append({
                'event_id': event_id,
                'event_title': event_data['event_title'],
                'event_slug': event_data['event_slug'],
                'markets_count': len(event_markets),
                'total_volume': event_volume,
                'total_liquidity': event_liquidity,
                'markets': sorted(event_markets, key=lambda x: float(x.get('volumeNum', 0) or 0), reverse=True)
            })
        
        # æŒ‰äº‹ä»¶äº¤æ˜“é‡æ’åº
        events_stats.sort(key=lambda x: x['total_volume'], reverse=True)
        
        report = {
            "timestamp": current_time.isoformat(),
            "search_term": search_term,
            "total_markets": len(markets),
            "markets_with_events": len(markets_with_events),
            "markets_without_events": len(markets_without_events),
            "total_events": len(events_info),
            "total_volume": total_volume,
            "total_liquidity": total_liquidity,
            "categories": categories,
            "time_ranges": time_ranges,
            "events_stats": events_stats,
            "top_markets": [],
            "standalone_markets": []
        }
        
        # æ·»åŠ å‰20ä¸ªäº¤æ˜“é‡æœ€å¤§çš„å¸‚åœº
        for i, market in enumerate(sorted_markets[:20]):
            end_date = market.get('endDate', '')
            time_remaining = "Unknown"
            
            if end_date:
                try:
                    end_time = date_parser.parse(end_date)
                    current_time_tz = datetime.now(end_time.tzinfo) if end_time.tzinfo else datetime.now()
                    time_diff = end_time - current_time_tz
                    
                    if time_diff.total_seconds() > 0:
                        days = time_diff.days
                        hours = time_diff.seconds // 3600
                        if days > 0:
                            time_remaining = f"{days}å¤©{hours}å°æ—¶"
                        else:
                            time_remaining = f"{hours}å°æ—¶"
                    else:
                        time_remaining = "å·²è¿‡æœŸ"
                except:
                    pass
            
            # è·å–äº‹ä»¶ä¿¡æ¯
            event_info = market.get('event_info', {})
            
            report["top_markets"].append({
                "rank": i + 1,
                "id": market.get('id', ''),
                "question": market.get('question', '')[:100],
                "category": market.get('category', ''),
                "endDate": end_date,
                "time_remaining": time_remaining,
                "volume": market.get('volumeNum', 0),
                "liquidity": market.get('liquidityNum', 0),
                "event_id": event_info.get('event_id', ''),
                "event_title": event_info.get('event_title', '')[:50] if event_info.get('event_title') else ''
            })
        
        # æ·»åŠ ç‹¬ç«‹å¸‚åœºï¼ˆæ— äº‹ä»¶ä¿¡æ¯çš„å¸‚åœºï¼‰
        sorted_standalone = sorted(markets_without_events, key=lambda x: float(x.get('volumeNum', 0) or 0), reverse=True)
        for i, market in enumerate(sorted_standalone[:10]):
            end_date = market.get('endDate', '')
            time_remaining = "Unknown"
            
            if end_date:
                try:
                    end_time = date_parser.parse(end_date)
                    current_time_tz = datetime.now(end_time.tzinfo) if end_time.tzinfo else datetime.now()
                    time_diff = end_time - current_time_tz
                    
                    if time_diff.total_seconds() > 0:
                        days = time_diff.days
                        hours = time_diff.seconds // 3600
                        if days > 0:
                            time_remaining = f"{days}å¤©{hours}å°æ—¶"
                        else:
                            time_remaining = f"{hours}å°æ—¶"
                    else:
                        time_remaining = "å·²è¿‡æœŸ"
                except:
                    pass
            
            report["standalone_markets"].append({
                "rank": i + 1,
                "id": market.get('id', ''),
                "question": market.get('question', '')[:100],
                "category": market.get('category', ''),
                "endDate": end_date,
                "time_remaining": time_remaining,
                "volume": market.get('volumeNum', 0),
                "liquidity": market.get('liquidityNum', 0)
            })
        
        return report

    def print_markets_summary(self, markets: List[Dict], search_term: str):
        """
        æ‰“å°å¸‚åœºæ‘˜è¦ - åˆ†åˆ«å±•ç¤ºäº‹ä»¶å’Œç‹¬ç«‹å¸‚åœº
        """
        report = self.generate_markets_report(markets, search_term)
        
        print("\n" + "="*80)
        print(f"ğŸ·ï¸  æ ‡ç­¾å¸‚åœºæŠ¥å‘Š - '{search_term}'")
        print("="*80)
        print(f"ğŸ• æŠ¥å‘Šæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¯ å‘ç°å¸‚åœº: {report['total_markets']} ä¸ª")
        print(f"   â””â”€ å…³è”äº‹ä»¶çš„å¸‚åœº: {report['markets_with_events']} ä¸ª")
        print(f"   â””â”€ ç‹¬ç«‹å¸‚åœº: {report['markets_without_events']} ä¸ª")
        print(f"ğŸª å‘ç°äº‹ä»¶: {report['total_events']} ä¸ª")
        print(f"ğŸ’° æ€»äº¤æ˜“é‡: ${report['total_volume']:,.2f}")
        print(f"ğŸ’§ æ€»æµåŠ¨æ€§: ${report['total_liquidity']:,.2f}")
        
        if report['categories']:
            print(f"\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
            for category, count in sorted(report['categories'].items(), key=lambda x: x[1], reverse=True):
                if category and category != 'Unknown':
                    print(f"   {category}: {count} ä¸ªå¸‚åœº")
        
        if report['time_ranges']:
            print(f"\nâ° ç»“æŸæ—¶é—´åˆ†å¸ƒ:")
            for time_range, count in report['time_ranges'].items():
                if count > 0:
                    print(f"   {time_range}: {count} ä¸ªå¸‚åœº")
        
        # æ˜¾ç¤ºäº‹ä»¶éƒ¨åˆ†
        if report['events_stats']:
            print(f"\n" + "="*60)
            print(f"ğŸª äº‹ä»¶éƒ¨åˆ† - å…± {len(report['events_stats'])} ä¸ªäº‹ä»¶")
            print("="*60)
            
            for i, event in enumerate(report['events_stats'][:10]):  # æ˜¾ç¤ºå‰10ä¸ªäº‹ä»¶
                print(f"\nğŸ“… äº‹ä»¶ {i+1}: {event['event_title']}")
                print(f"   äº‹ä»¶ID: {event['event_id']}")
                print(f"   å¸‚åœºæ•°é‡: {event['markets_count']} ä¸ª")
                print(f"   æ€»äº¤æ˜“é‡: ${event['total_volume']:,.2f}")
                print(f"   æ€»æµåŠ¨æ€§: ${event['total_liquidity']:,.2f}")
                
                # æ˜¾ç¤ºè¯¥äº‹ä»¶ä¸‹çš„å‰3ä¸ªå¸‚åœº
                top_event_markets = event['markets'][:3]
                if top_event_markets:
                    print(f"   ğŸ”¥ çƒ­é—¨å¸‚åœº:")
                    for j, market in enumerate(top_event_markets):
                        end_date = market.get('endDate', '')
                        time_remaining = "Unknown"
                        if end_date:
                            try:
                                end_time = date_parser.parse(end_date)
                                current_time_tz = datetime.now(end_time.tzinfo) if end_time.tzinfo else datetime.now()
                                time_diff = end_time - current_time_tz
                                if time_diff.total_seconds() > 0:
                                    days = time_diff.days
                                    hours = time_diff.seconds // 3600
                                    if days > 0:
                                        time_remaining = f"{days}å¤©{hours}å°æ—¶"
                                    else:
                                        time_remaining = f"{hours}å°æ—¶"
                                else:
                                    time_remaining = "å·²è¿‡æœŸ"
                            except:
                                pass
                        
                        volume = float(market.get('volumeNum', 0) or 0)
                        question = market.get('question', '')[:80]
                        print(f"      {j+1}. [{time_remaining}] {question}")
                        print(f"         ID: {market.get('id', '')} | äº¤æ˜“é‡: ${volume:,.0f}")
            
            if len(report['events_stats']) > 10:
                print(f"\n   ... è¿˜æœ‰ {len(report['events_stats']) - 10} ä¸ªäº‹ä»¶")
        
        # æ˜¾ç¤ºç‹¬ç«‹å¸‚åœºéƒ¨åˆ†
        if report['standalone_markets']:
            print(f"\n" + "="*60)
            print(f"ğŸª ç‹¬ç«‹å¸‚åœºéƒ¨åˆ† - å…± {len(report['standalone_markets'])} ä¸ªå¸‚åœº")
            print("="*60)
            
            for market in report['standalone_markets']:
                print(f"   {market['rank']:2d}. [{market['time_remaining']}] {market['question']}")
                print(f"       ID: {market['id']} | äº¤æ˜“é‡: ${market['volume']:,.0f}")
        
        # æ˜¾ç¤ºæ€»ä½“æ’åï¼ˆæ‰€æœ‰å¸‚åœºæ··åˆï¼‰
        if report['top_markets']:
            print(f"\n" + "="*60)
            print(f"ğŸ† æ€»ä½“äº¤æ˜“é‡æ’å (å‰{min(10, len(report['top_markets']))}ä¸ª)")
            print("="*60)
            for market in report['top_markets'][:10]:
                event_info = f" [äº‹ä»¶: {market['event_title']}]" if market.get('event_title') else " [ç‹¬ç«‹å¸‚åœº]"
                print(f"   {market['rank']:2d}. [{market['time_remaining']}] {market['question']}{event_info}")
                print(f"       ID: {market['id']} | äº¤æ˜“é‡: ${market['volume']:,.0f}")
        
        print("="*80)

    def run_tag_sync(self, tag_ids: List[str] = None, keywords: List[str] = None, 
                     save_to_file: bool = True, search_method: str = 'both') -> Dict[str, Any]:
        """
        è¿è¡Œæ ‡ç­¾å¸‚åœºåŒæ­¥
        
        Args:
            tag_ids: æ ‡ç­¾IDåˆ—è¡¨
            keywords: å…³é”®è¯åˆ—è¡¨
            save_to_file: æ˜¯å¦ä¿å­˜åˆ°æ–‡ä»¶
            search_method: å…³é”®è¯æœç´¢æ–¹æ³• ('event', 'direct', 'both')
            
        Returns:
            Dict: åŒæ­¥ç»“æœ
        """
        start_time = datetime.now()
        logger.info(f"ğŸš€ å¼€å§‹æ ‡ç­¾å¸‚åœºåŒæ­¥")
        
        try:
            all_markets = []
            search_terms = []
            
            # é€šè¿‡æ ‡ç­¾IDæœç´¢
            if tag_ids:
                tag_markets = self.search_markets_by_multiple_tags(tag_ids)
                all_markets.extend(tag_markets)
                search_terms.extend(tag_ids)
            
            # é€šè¿‡å…³é”®è¯æœç´¢
            if keywords:
                seen_ids = {m.get('id') for m in all_markets}
                for keyword in keywords:
                    keyword_markets = self.search_markets_by_keyword(keyword, search_method=search_method)
                    # å»é‡æ·»åŠ 
                    for market in keyword_markets:
                        if market.get('id') not in seen_ids:
                            all_markets.append(market)
                            seen_ids.add(market.get('id'))
                    search_terms.append(keyword)
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæœç´¢æ¡ä»¶ï¼Œè¿”å›é”™è¯¯
            if not tag_ids and not keywords:
                return {
                    "success": False,
                    "error": "å¿…é¡»æŒ‡å®šè‡³å°‘ä¸€ä¸ªæ ‡ç­¾IDæˆ–å…³é”®è¯",
                    "markets_count": 0
                }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            csv_file = None
            if save_to_file and all_markets:
                search_name = "_".join(search_terms[:3])  # æœ€å¤šä½¿ç”¨å‰3ä¸ªæœç´¢è¯
                csv_file = self.save_markets_data(all_markets, search_name)
            
            # ç”ŸæˆæŠ¥å‘Š
            search_term_str = ", ".join(search_terms)
            report = self.generate_markets_report(all_markets, search_term_str)
            
            # ä¿å­˜JSONæŠ¥å‘Š
            if save_to_file:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                safe_search_name = "".join(c for c in search_term_str if c.isalnum() or c in ('-', '_', ' ')).replace(' ', '_')
                json_file = os.path.join(self.data_dir, "reports", f"tag_report_{safe_search_name}_{timestamp}.json")
                with open(json_file, "w", encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False)
                logger.info(f"ğŸ“Š æŠ¥å‘Šå·²ä¿å­˜åˆ°: {json_file}")
            
            # æ‰“å°æ‘˜è¦
            self.print_markets_summary(all_markets, search_term_str)
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            return {
                "success": True,
                "markets_count": len(all_markets),
                "csv_file": csv_file,
                "duration_seconds": duration,
                "report": report
            }
            
        except Exception as e:
            logger.error(f"æ ‡ç­¾å¸‚åœºåŒæ­¥å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "markets_count": 0
            }

def main():
    parser = argparse.ArgumentParser(description="æ ‡ç­¾å¸‚åœºåŒæ­¥å™¨ - é€šç”¨çš„æ ‡ç­¾IDå’Œå…³é”®è¯å¸‚åœºæœç´¢å·¥å…·")
    parser.add_argument("--data-dir", default="./data", help="æ•°æ®ç›®å½•")
    parser.add_argument("--tag-ids", nargs='+', help="è¦æœç´¢çš„æ ‡ç­¾IDåˆ—è¡¨")
    parser.add_argument("--keywords", nargs='+', help="è¦æœç´¢çš„å…³é”®è¯åˆ—è¡¨")
    parser.add_argument("--search-method", choices=['event', 'direct', 'both'], default='both',
                       help="å…³é”®è¯æœç´¢æ–¹æ³•: event=é€šè¿‡äº‹ä»¶æœç´¢, direct=ç›´æ¥æœç´¢å¸‚åœº, both=ä¸¤ç§æ–¹æ³•éƒ½ç”¨")
    parser.add_argument("--list-tags", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ ‡ç­¾ID")
    parser.add_argument("--sync-tags", action="store_true", help="åŒæ­¥æ‰€æœ‰å¯ç”¨æ ‡ç­¾åˆ°æ–‡ä»¶")
    parser.add_argument("--test-connection", action="store_true", help="æµ‹è¯•ç½‘ç»œè¿æ¥")
    parser.add_argument("--no-save", action="store_true", help="ä¸ä¿å­˜åˆ°æ–‡ä»¶ï¼Œä»…æ˜¾ç¤ºç»“æœ")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ—¥å¿—")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    syncer = TagMarketsSync(data_dir=args.data_dir)
    
    # å¦‚æœç”¨æˆ·è¦æ±‚æµ‹è¯•è¿æ¥
    if args.test_connection:
        success = syncer.run_connection_test()
        if success:
            print("\nâœ… è¿æ¥æµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨ç¨‹åº")
        else:
            print("\nâŒ è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®")
        return
    
    # å¦‚æœç”¨æˆ·è¦æ±‚åŒæ­¥æ‰€æœ‰æ ‡ç­¾
    if args.sync_tags:
        result = syncer.sync_all_tags()
        if result["success"]:
            print(f"\nâœ… æ ‡ç­¾åŒæ­¥å®Œæˆ!")
            print(f"   åŒæ­¥æ ‡ç­¾: {result['tags_count']} ä¸ª")
            print(f"   è€—æ—¶: {result['duration_seconds']:.1f} ç§’")
            print(f"   æ•°æ®æ–‡ä»¶: {result['csv_file']}")
        else:
            print(f"\nâŒ æ ‡ç­¾åŒæ­¥å¤±è´¥: {result.get('error', 'Unknown error')}")
            if "Connection" in str(result.get('error', '')):
                syncer.print_connection_troubleshooting()
        return
    
    # å¦‚æœç”¨æˆ·è¦æ±‚åˆ—å‡ºæ ‡ç­¾
    if args.list_tags:
        syncer.get_available_tags()
        return
    
    # æ£€æŸ¥å‚æ•°
    if not args.tag_ids and not args.keywords:
        print("âŒ é”™è¯¯: å¿…é¡»æŒ‡å®šè‡³å°‘ä¸€ä¸ª --tag-ids æˆ– --keywords å‚æ•°")
        print("\nç¤ºä¾‹ç”¨æ³•:")
        print("  python3 sync/tag_markets_sync.py --test-connection  # æµ‹è¯•ç½‘ç»œè¿æ¥")
        print("  python3 sync/tag_markets_sync.py --list-tags        # æŸ¥çœ‹å¯ç”¨æ ‡ç­¾ID")
        print("  python3 sync/tag_markets_sync.py --sync-tags        # åŒæ­¥æ‰€æœ‰æ ‡ç­¾åˆ°æ–‡ä»¶")
        print("  python3 sync/tag_markets_sync.py --tag-ids 180 241")
        print("  python3 sync/tag_markets_sync.py --keywords bitcoin crypto")
        print("  python3 sync/tag_markets_sync.py --keywords bitcoin --search-method event")
        print("  python3 sync/tag_markets_sync.py --tag-ids 180 --keywords football --search-method both")
        return
    
    result = syncer.run_tag_sync(
        tag_ids=args.tag_ids,
        keywords=args.keywords,
        save_to_file=not args.no_save,
        search_method=args.search_method
    )
    
    if result["success"]:
        print(f"\nâœ… æ ‡ç­¾å¸‚åœºåŒæ­¥å®Œæˆ!")
        print(f"   å‘ç°å¸‚åœº: {result['markets_count']} ä¸ª")
        print(f"   æœç´¢æ–¹æ³•: {args.search_method}")
        print(f"   è€—æ—¶: {result['duration_seconds']:.1f} ç§’")
        if result.get("csv_file"):
            print(f"   æ•°æ®æ–‡ä»¶: {result['csv_file']}")
    else:
        print(f"\nâŒ æ ‡ç­¾å¸‚åœºåŒæ­¥å¤±è´¥: {result.get('error', 'Unknown error')}")
        if "Connection" in str(result.get('error', '')):
            syncer.print_connection_troubleshooting()

if __name__ == "__main__":
    main()