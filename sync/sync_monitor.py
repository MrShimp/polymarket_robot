#!/usr/bin/env python3
"""
Polymarket åŒæ­¥ç›‘æ§å™¨
ç›‘æ§åŒæ­¥çŠ¶æ€ï¼Œç”ŸæˆæŠ¥å‘Šï¼Œç®¡ç†æ•°æ®æ–‡ä»¶
"""

import os
import json
import time
import glob
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import pandas as pd
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SyncMonitor:
    """åŒæ­¥ç›‘æ§å™¨"""
    
    def __init__(self, data_dir: str = "./data"):
        self.data_dir = data_dir
        self.tag_dir = os.path.join(data_dir, "tag")
        self.reports_dir = os.path.join(data_dir, "reports")
    
    def get_sync_status(self) -> Dict:
        """è·å–åŒæ­¥çŠ¶æ€"""
        status = {
            'last_sync_time': None,
            'total_tags': 0,
            'total_events': 0,
            'total_markets': 0,
            'tag_details': {},
            'data_freshness': None,
            'disk_usage': self.get_disk_usage()
        }
        
        if not os.path.exists(self.tag_dir):
            return status
        
        # ç»Ÿè®¡æ ‡ç­¾ç›®å½•
        tag_dirs = [d for d in os.listdir(self.tag_dir) 
                   if os.path.isdir(os.path.join(self.tag_dir, d))]
        
        status['total_tags'] = len(tag_dirs)
        
        latest_sync_time = None
        
        # åˆ†ææ¯ä¸ªæ ‡ç­¾ç›®å½•
        for tag_name in tag_dirs:
            tag_path = os.path.join(self.tag_dir, tag_name)
            tag_info = self.analyze_tag_directory(tag_path)
            status['tag_details'][tag_name] = tag_info
            
            # æ›´æ–°æ€»è®¡æ•°
            status['total_events'] += tag_info['events_count']
            status['total_markets'] += tag_info['markets_count']
            
            # æ‰¾åˆ°æœ€æ–°çš„åŒæ­¥æ—¶é—´
            if tag_info['last_update'] and (not latest_sync_time or tag_info['last_update'] > latest_sync_time):
                latest_sync_time = tag_info['last_update']
        
        status['last_sync_time'] = latest_sync_time.isoformat() if latest_sync_time else None
        
        # è®¡ç®—æ•°æ®æ–°é²œåº¦
        if latest_sync_time:
            age_hours = (datetime.now() - latest_sync_time).total_seconds() / 3600
            status['data_freshness'] = {
                'age_hours': age_hours,
                'status': 'fresh' if age_hours < 6 else 'stale' if age_hours < 24 else 'old'
            }
        
        return status
    
    def analyze_tag_directory(self, tag_path: str) -> Dict:
        """åˆ†ææ ‡ç­¾ç›®å½•"""
        info = {
            'events_count': 0,
            'markets_count': 0,
            'last_update': None,
            'file_count': 0,
            'total_size_mb': 0
        }
        
        if not os.path.exists(tag_path):
            return info
        
        files = os.listdir(tag_path)
        info['file_count'] = len(files)
        
        # è®¡ç®—æ€»å¤§å°
        total_size = 0
        latest_time = None
        
        for filename in files:
            file_path = os.path.join(tag_path, filename)
            if os.path.isfile(file_path):
                # æ–‡ä»¶å¤§å°
                total_size += os.path.getsize(file_path)
                
                # æœ€åä¿®æ”¹æ—¶é—´
                mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                if not latest_time or mtime > latest_time:
                    latest_time = mtime
                
                # ç»Ÿè®¡äº‹ä»¶å’Œå¸‚åœºæ•°é‡ï¼ˆä»æœ€æ–°æ–‡ä»¶ï¼‰
                if filename.startswith('events_') and filename.endswith('.csv'):
                    try:
                        df = pd.read_csv(file_path)
                        info['events_count'] = len(df)
                    except:
                        pass
                elif filename.startswith('markets_') and filename.endswith('.csv'):
                    try:
                        df = pd.read_csv(file_path)
                        info['markets_count'] = len(df)
                    except:
                        pass
        
        info['total_size_mb'] = total_size / (1024 * 1024)
        info['last_update'] = latest_time
        
        return info
    
    def get_disk_usage(self) -> Dict:
        """è·å–ç£ç›˜ä½¿ç”¨æƒ…å†µ"""
        usage = {
            'total_size_mb': 0,
            'tag_dir_size_mb': 0,
            'reports_dir_size_mb': 0
        }
        
        # è®¡ç®—æ ‡ç­¾ç›®å½•å¤§å°
        if os.path.exists(self.tag_dir):
            usage['tag_dir_size_mb'] = self.get_directory_size(self.tag_dir)
        
        # è®¡ç®—æŠ¥å‘Šç›®å½•å¤§å°
        if os.path.exists(self.reports_dir):
            usage['reports_dir_size_mb'] = self.get_directory_size(self.reports_dir)
        
        usage['total_size_mb'] = usage['tag_dir_size_mb'] + usage['reports_dir_size_mb']
        
        return usage
    
    def get_directory_size(self, directory: str) -> float:
        """è·å–ç›®å½•å¤§å°ï¼ˆMBï¼‰"""
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(directory):
            for filename in filenames:
                file_path = os.path.join(dirpath, filename)
                if os.path.isfile(file_path):
                    total_size += os.path.getsize(file_path)
        return total_size / (1024 * 1024)
    
    def get_latest_sync_report(self) -> Optional[Dict]:
        """è·å–æœ€æ–°çš„åŒæ­¥æŠ¥å‘Š"""
        if not os.path.exists(self.reports_dir):
            return None
        
        # æŸ¥æ‰¾æœ€æ–°çš„æŠ¥å‘Šæ–‡ä»¶
        report_files = glob.glob(os.path.join(self.reports_dir, "sync_report_*.json"))
        if not report_files:
            return None
        
        latest_file = max(report_files, key=os.path.getmtime)
        
        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¯»å–æŠ¥å‘Šæ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def generate_status_report(self) -> str:
        """ç”ŸæˆçŠ¶æ€æŠ¥å‘Š"""
        status = self.get_sync_status()
        latest_report = self.get_latest_sync_report()
        
        freshness_text = 'N/A'
        if status['data_freshness']:
            age_hours = status['data_freshness']['age_hours']
            freshness_text = f"{status['data_freshness']['status']} ({age_hours:.1f}hå‰)"
        
        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    åŒæ­¥çŠ¶æ€ç›‘æ§æŠ¥å‘Š                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ ğŸ“Š å½“å‰çŠ¶æ€                                                  â•‘
â•‘   æœ€ååŒæ­¥: {status['last_sync_time'] or 'N/A'}             â•‘
â•‘   æ•°æ®æ–°é²œåº¦: {freshness_text}                               â•‘
â•‘   æ ‡ç­¾æ•°é‡: {status['total_tags']}                          â•‘
â•‘   äº‹ä»¶æ€»æ•°: {status['total_events']:,}                      â•‘
â•‘   å¸‚åœºæ€»æ•°: {status['total_markets']:,}                     â•‘
â•‘                                                              â•‘
â•‘ ğŸ’¾ å­˜å‚¨ä½¿ç”¨                                                  â•‘
â•‘   æ€»å¤§å°: {status['disk_usage']['total_size_mb']:.1f} MB     â•‘
â•‘   æ ‡ç­¾æ•°æ®: {status['disk_usage']['tag_dir_size_mb']:.1f} MB â•‘
â•‘   æŠ¥å‘Šæ•°æ®: {status['disk_usage']['reports_dir_size_mb']:.1f} MB â•‘
â•‘                                                              â•‘
â•‘ ğŸ·ï¸  çƒ­é—¨æ ‡ç­¾                                                 â•‘"""
        
        # æŒ‰äº‹ä»¶æ•°é‡æ’åºæ ‡ç­¾
        sorted_tags = sorted(
            status['tag_details'].items(),
            key=lambda x: x[1]['events_count'],
            reverse=True
        )
        
        for i, (tag_name, tag_info) in enumerate(sorted_tags[:5], 1):
            report += f"""
â•‘   {i}. {tag_name}: {tag_info['events_count']} äº‹ä»¶, {tag_info['markets_count']} å¸‚åœº â•‘"""
        
        if latest_report:
            sync_info = latest_report.get('sync_info', {})
            report += f"""
â•‘                                                              â•‘
â•‘ ğŸ“ˆ æœ€è¿‘åŒæ­¥ç»Ÿè®¡                                              â•‘
â•‘   åŒæ­¥è€—æ—¶: {sync_info.get('duration_seconds', 0):.1f} ç§’    â•‘
â•‘   å¤„ç†äº‹ä»¶: {sync_info.get('total_events', 0):,}            â•‘
â•‘   å¤„ç†å¸‚åœº: {sync_info.get('total_markets', 0):,}           â•‘"""
        
        report += f"""
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        
        return report.strip()
    
    def check_data_quality(self) -> Dict:
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        issues = []
        warnings = []
        
        status = self.get_sync_status()
        
        # æ£€æŸ¥æ•°æ®æ–°é²œåº¦
        if status['data_freshness']:
            age_hours = status['data_freshness']['age_hours']
            if age_hours > 24:
                issues.append(f"æ•°æ®è¿‡æœŸ: {age_hours:.1f} å°æ—¶æœªæ›´æ–°")
            elif age_hours > 12:
                warnings.append(f"æ•°æ®è¾ƒæ—§: {age_hours:.1f} å°æ—¶æœªæ›´æ–°")
        
        # æ£€æŸ¥æ ‡ç­¾æ•°é‡
        if status['total_tags'] == 0:
            issues.append("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ ‡ç­¾æ•°æ®")
        elif status['total_tags'] < 5:
            warnings.append(f"æ ‡ç­¾æ•°é‡è¾ƒå°‘: åªæœ‰ {status['total_tags']} ä¸ªæ ‡ç­¾")
        
        # æ£€æŸ¥äº‹ä»¶æ•°é‡
        if status['total_events'] == 0:
            issues.append("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•äº‹ä»¶æ•°æ®")
        elif status['total_events'] < 10:
            warnings.append(f"äº‹ä»¶æ•°é‡è¾ƒå°‘: åªæœ‰ {status['total_events']} ä¸ªäº‹ä»¶")
        
        # æ£€æŸ¥ç£ç›˜ä½¿ç”¨
        total_size = status['disk_usage']['total_size_mb']
        if total_size > 1000:  # 1GB
            warnings.append(f"ç£ç›˜ä½¿ç”¨è¾ƒå¤§: {total_size:.1f} MB")
        
        # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
        tag_details = status['tag_details']
        if tag_details:
            event_counts = [info['events_count'] for info in tag_details.values()]
            max_events = max(event_counts)
            min_events = min(event_counts)
            
            if max_events > min_events * 10:  # åˆ†å¸ƒä¸å‡
                warnings.append("æ ‡ç­¾äº‹ä»¶åˆ†å¸ƒä¸å‡åŒ€")
        
        return {
            'issues': issues,
            'warnings': warnings,
            'quality_score': max(0, 100 - len(issues) * 30 - len(warnings) * 10)
        }
    
    def cleanup_old_data(self, days: int = 7) -> Dict:
        """æ¸…ç†æ—§æ•°æ®"""
        cleaned_files = 0
        freed_space_mb = 0
        cutoff_time = datetime.now() - timedelta(days=days)
        
        # æ¸…ç†æ ‡ç­¾ç›®å½•
        if os.path.exists(self.tag_dir):
            for tag_name in os.listdir(self.tag_dir):
                tag_path = os.path.join(self.tag_dir, tag_name)
                if os.path.isdir(tag_path):
                    for filename in os.listdir(tag_path):
                        file_path = os.path.join(tag_path, filename)
                        if os.path.isfile(file_path):
                            mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                            if mtime < cutoff_time:
                                file_size = os.path.getsize(file_path) / (1024 * 1024)
                                os.remove(file_path)
                                cleaned_files += 1
                                freed_space_mb += file_size
        
        # æ¸…ç†æŠ¥å‘Šç›®å½•
        if os.path.exists(self.reports_dir):
            for filename in os.listdir(self.reports_dir):
                file_path = os.path.join(self.reports_dir, filename)
                if os.path.isfile(file_path):
                    mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                    if mtime < cutoff_time:
                        file_size = os.path.getsize(file_path) / (1024 * 1024)
                        os.remove(file_path)
                        cleaned_files += 1
                        freed_space_mb += file_size
        
        return {
            'cleaned_files': cleaned_files,
            'freed_space_mb': freed_space_mb
        }
    
    def export_tag_summary(self, tag_name: str) -> Optional[str]:
        """å¯¼å‡ºæ ‡ç­¾æ‘˜è¦"""
        tag_path = os.path.join(self.tag_dir, tag_name)
        if not os.path.exists(tag_path):
            return None
        
        # æŸ¥æ‰¾æœ€æ–°çš„æ‘˜è¦æ–‡ä»¶
        summary_files = glob.glob(os.path.join(tag_path, "summary_*.json"))
        if not summary_files:
            return None
        
        latest_summary = max(summary_files, key=os.path.getmtime)
        
        try:
            with open(latest_summary, 'r', encoding='utf-8') as f:
                summary_data = json.load(f)
            
            # ç”Ÿæˆå¯è¯»çš„æ‘˜è¦
            report = f"""
æ ‡ç­¾: {tag_name}
äº‹ä»¶æ•°é‡: {summary_data.get('events_count', 0)}
å¸‚åœºæ•°é‡: {summary_data.get('markets_count', 0)}
æ€»äº¤æ˜“é‡: ${summary_data.get('total_volume', 0):,}
æ€»æµåŠ¨æ€§: ${summary_data.get('total_liquidity', 0):,}
åŒæ­¥æ—¶é—´: {summary_data.get('sync_timestamp', 'N/A')}

çƒ­é—¨äº‹ä»¶:
"""
            
            for i, event in enumerate(summary_data.get('top_events', []), 1):
                report += f"{i}. {event.get('title', 'N/A')} (${event.get('volume', 0):,})\n"
            
            return report.strip()
            
        except Exception as e:
            logger.error(f"è¯»å–æ‘˜è¦æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def monitor_real_time(self, interval: int = 300):
        """å®æ—¶ç›‘æ§"""
        logger.info(f"å¼€å§‹å®æ—¶ç›‘æ§ï¼Œåˆ·æ–°é—´éš”: {interval} ç§’")
        
        while True:
            try:
                # æ¸…å±å¹¶æ˜¾ç¤ºçŠ¶æ€
                os.system('clear' if os.name == 'posix' else 'cls')
                
                print(self.generate_status_report())
                
                # æ£€æŸ¥æ•°æ®è´¨é‡
                quality = self.check_data_quality()
                if quality['issues']:
                    print(f"\nâŒ å‘ç°é—®é¢˜:")
                    for issue in quality['issues']:
                        print(f"   - {issue}")
                
                if quality['warnings']:
                    print(f"\nâš ï¸  è­¦å‘Š:")
                    for warning in quality['warnings']:
                        print(f"   - {warning}")
                
                print(f"\nğŸ“Š æ•°æ®è´¨é‡è¯„åˆ†: {quality['quality_score']}/100")
                print(f"\næœ€åæ›´æ–°: {datetime.now().strftime('%H:%M:%S')}")
                print("æŒ‰ Ctrl+C é€€å‡ºç›‘æ§")
                
                time.sleep(interval)
                
            except KeyboardInterrupt:
                logger.info("ç›‘æ§å·²åœæ­¢")
                break
            except Exception as e:
                logger.error(f"ç›‘æ§é”™è¯¯: {e}")
                time.sleep(10)

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='PolymarketåŒæ­¥ç›‘æ§å™¨')
    parser.add_argument('--action', choices=['status', 'monitor', 'cleanup', 'quality', 'export'], 
                       default='status', help='æ‰§è¡Œçš„æ“ä½œ')
    parser.add_argument('--data-dir', default='./data', help='æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--interval', type=int, default=300, help='ç›‘æ§åˆ·æ–°é—´éš”(ç§’)')
    parser.add_argument('--cleanup-days', type=int, default=7, help='æ¸…ç†Nå¤©å‰çš„æ–‡ä»¶')
    parser.add_argument('--tag', help='å¯¼å‡ºç‰¹å®šæ ‡ç­¾çš„æ‘˜è¦')
    
    args = parser.parse_args()
    
    monitor = SyncMonitor(data_dir=args.data_dir)
    
    if args.action == 'status':
        print(monitor.generate_status_report())
    
    elif args.action == 'monitor':
        monitor.monitor_real_time(interval=args.interval)
    
    elif args.action == 'cleanup':
        result = monitor.cleanup_old_data(days=args.cleanup_days)
        print(f"æ¸…ç†å®Œæˆ:")
        print(f"  åˆ é™¤æ–‡ä»¶: {result['cleaned_files']} ä¸ª")
        print(f"  é‡Šæ”¾ç©ºé—´: {result['freed_space_mb']:.1f} MB")
    
    elif args.action == 'quality':
        quality = monitor.check_data_quality()
        print(f"æ•°æ®è´¨é‡è¯„åˆ†: {quality['quality_score']}/100")
        
        if quality['issues']:
            print(f"\nâŒ é—®é¢˜:")
            for issue in quality['issues']:
                print(f"   - {issue}")
        
        if quality['warnings']:
            print(f"\nâš ï¸  è­¦å‘Š:")
            for warning in quality['warnings']:
                print(f"   - {warning}")
    
    elif args.action == 'export':
        if not args.tag:
            print("è¯·æŒ‡å®šè¦å¯¼å‡ºçš„æ ‡ç­¾åç§° (--tag)")
            return
        
        summary = monitor.export_tag_summary(args.tag)
        if summary:
            print(summary)
        else:
            print(f"æœªæ‰¾åˆ°æ ‡ç­¾ '{args.tag}' çš„æ‘˜è¦æ•°æ®")

if __name__ == "__main__":
    main()